# LLM-Paper-Daily

## Awesome LLM Research

Welcome to the **Awesome LLM Research** repository! This project curates a list of high-quality resources related to LLM Research, including research papers, tools, libraries, and more.

## Daily Updates

### 2024-10-09

### 1. [Misinformation with Legal Consequences (MisLC): A New Task Towards Harnessing Societal Harm of Misinformation](https://arxiv.org/pdf/2410.03829)
**Summary**: The paper introduces a new task called Misinformation with Legal Consequence (MisLC), which focuses on identifying misinformation that has legal implications and societal harm. It proposes a two-step dataset curation method and highlights the challenges in replicating expert performance, even with advanced language models.

### 2. [Reasoning Elicitation in Language Models via Counterfactual Feedback](https://arxiv.org/pdf/2410.03767)
**Summary**: The paper introduces new metrics to evaluate the reasoning capabilities of language models, particularly in counterfactual scenarios, and proposes fine-tuning methods to enhance these abilities. The study evaluates the fine-tuned models across various reasoning tasks, demonstrating improved generalization and performance compared to base models.

### 3. [Determine-Then-Ensemble: Necessity of Top-k Union for Large Language Model Ensembling](https://arxiv.org/pdf/2410.03777)
**Summary**: The paper investigates the challenges of ensembling large language models (LLMs) and identifies model compatibility, vocabulary size, and response style as critical factors for effective ensemble performance. It introduces a novel approach called UniTE, which focuses on the union of the top-k tokens from each model to efficiently combine them, avoiding full vocabulary alignment and reducing computational costs. Extensive evaluations show that UniTE significantly improves performance over existing methods.

### 4. [Self-Powered LLM Modality Expansion for Large Speech-Text Models](https://arxiv.org/pdf/2410.03798)
**Summary**: The paper introduces a self-powered approach to enhance large speech-text models (LSMs) by addressing the issue of speech anchor bias, where models over-rely on speech inputs. By using augmented automatic speech recognition data generated by the model itself, the method improves instruction tuning and better integrates speech and text modalities, as demonstrated in various speech-based tasks.

### 5. [Searching for Best Practices in Medical Transcription with Large Language Model](https://arxiv.org/pdf/2410.03797)
**Summary**: The paper presents a novel approach using a Large Language Model (LLM) to improve the accuracy of medical transcription, particularly for monologues with specialized terminology and Indian accents. By integrating advanced language modeling techniques, the system significantly reduces Word Error Rates (WER) and enhances the recognition of critical medical terms, demonstrating potential to streamline clinical documentation processes with high accuracy.

### 6. [Hidden in Plain Text: Emergence & Mitigation of Steganographic Collusion in LLMs](https://arxiv.org/pdf/2410.03768)
**Summary**: The paper explores the emergence of steganographic collusion in large language models (LLMs) and demonstrates that such collusion can arise from optimization pressures, making it difficult to detect and mitigate. The authors propose two reinforcement learning methods to elicit sophisticated steganographic text and find that it is robust to both passive and active oversight techniques, highlighting the need for innovative risk mitigation strategies.

### 7. [ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD](https://arxiv.org/pdf/2410.03845)
**Summary**: The paper introduces ORAssistant, a conversational assistant for the OpenROAD EDA tool, leveraging Retrieval-Augmented Generation (RAG) to enhance user assistance in tasks such as setup, decision-making, and flow automation. ORAssistant integrates multiple open-source tools and is built on Google Gemini, demonstrating improved performance and accuracy in early evaluations compared to non-fine-tuned Large Language Models.

### 8. [A Two-Stage Proactive Dialogue Generator for Efficient Clinical Information Collection Using Large Language Model](https://arxiv.org/pdf/2410.03770)
**Summary**: The paper introduces a two-stage proactive dialogue generator that automates the collection of clinical information during patient-doctor interactions, aiming to enhance the efficiency and accuracy of disease diagnosis. The system uses a large language model to simulate doctor-patient conversations, employing a two-stage recommendation structure and carefully designed ranking criteria to generate multi-round queries that mimic real doctor conversations, ensuring fluency, professionalism, and safety while gathering relevant diagnostic information.

### 9. [Basis Sharing: Cross-Layer Parameter Sharing for Large Language Model Compression](https://arxiv.org/pdf/2410.03765)
**Summary**: The paper introduces Basis Sharing, a method that leverages singular value decomposition (SVD) to share basis vectors across different layers of large language models (LLMs) for more effective compression. By decomposing weight matrices into shared basis vectors and unique coefficients, the approach reduces memory storage while maintaining performance, outperforming existing SVD-based and parameter-sharing techniques, especially at high compression ratios.

### 10. [SciSafeEval: A Comprehensive Benchmark for Safety Alignment of Large Language Models in Scientific Tasks](https://arxiv.org/pdf/2410.03769)
**Summary**: The paper introduces SciSafeEval, a comprehensive benchmark for evaluating the safety alignment of large language models (LLMs) in scientific tasks across various domains and representations, including molecular, protein, and genomic languages. The benchmark includes zero-shot, few-shot, and chain-of-thought evaluations, along with a 'jailbreak' feature to test LLMs' defenses against malicious intent, aiming to promote responsible development and deployment of LLMs in scientific research.

### 11. [Reward-RAG: Enhancing RAG with Reward Driven Supervision](https://arxiv.org/pdf/2410.03780)
**Summary**: The paper introduces Reward-RAG, a method that enhances Retrieval-Augmented Generation (RAG) models by using a reward model trained with CriticGPT to fine-tune the RAG encoder, aligning its outputs more closely with human preferences. This approach shows significant performance improvements across various domains, demonstrating the effectiveness of integrating reward models with RAG for superior natural language generation.

### 12. [FaithCAMERA: Construction of a Faithful Dataset for Ad Text Generation](https://arxiv.org/pdf/2410.03839)
**Summary**: The paper introduces FaithCAMERA, a new dataset for evaluating ad text generation that ensures faithfulness to the input document while maintaining informativeness. By refining the existing CAMERA dataset with the help of ad creators, FaithCAMERA addresses the issue of unfaithful information in reference texts. The study finds that while removing unfaithful entities improves faithfulness and informativeness at the entity level, it reduces both at the sentence level, highlighting the importance of ensuring the faithfulness of training data in future ad text generation research.

### 13. [Mixture of Attentions For Speculative Decoding](https://arxiv.org/pdf/2410.03804)
**Summary**: The paper introduces a Mixture of Attentions for Speculative Decoding (SD) to address limitations in current SD models, such as lack of on-policyness and partial observability. The proposed architecture achieves state-of-the-art speedups and improved acceptance lengths in single-device scenarios, and demonstrates superior performance in client-server deployments, including maintaining higher accuracy during network disconnections compared to existing methods.

### 14. [Precision Knowledge Editing: Enhancing Safety in Large Language Models](https://arxiv.org/pdf/2410.03772)
**Summary**: The paper introduces Precision Knowledge Editing (PKE), a technique that enhances the safety of large language models (LLMs) by more effectively identifying and modifying toxic parameter regions. PKE, which builds on existing knowledge editing methods, uses neuron weight tracking and activation pathway tracing to achieve finer granularity in managing toxic content. Experiments show that PKE significantly reduces the attack success rate across various models while maintaining overall performance, outperforming closed-source models in terms of safety.

### 15. [Using Prompts to Guide Large Language Models in Imitating a Real Person's Language Style](https://arxiv.org/pdf/2410.03848)
**Summary**: The study investigates the effectiveness of different prompts in guiding large language models (LLMs) to imitate a real person's language style. It compares the performance of three LLMs, with a focus on Llama 3, under various prompting strategies, including the Tree-of-Thoughts (ToT) method. The results indicate that Llama 3, when guided by the ToT prompting method, excels in imitating language styles, enabling it to create a conversational AI that accurately reflects the language style of a specific individual without altering its core parameters.

### 16. [Detecting Machine-Generated Long-Form Content with Latent-Space Variables](https://arxiv.org/pdf/2410.03856)
**Summary**: The paper introduces a novel method for detecting machine-generated long-form content by focusing on event transitions and topic sequences in a latent-space model, rather than relying on token-level distributions. This approach demonstrates a 31% improvement over existing detectors like DetectGPT, as it effectively distinguishes between human and machine-generated texts by exploiting the inherent differences in how modern language models like GPT-4 generate event triggers and transitions compared to humans.

### 17. [SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domains?](https://arxiv.org/pdf/2410.03859)
**Summary**: The paper introduces SWE-bench Multimodal (SWE-bench M) to evaluate AI systems' ability to handle visual and user-facing JavaScript software, revealing that existing top-performing systems struggle with this domain. It highlights SWE-agent's superior performance due to its language-agnostic features, resolving 12% of task instances compared to 6% for the next best system.

### 18. [You Know What I'm Saying -- Jailbreak Attack via Implicit Reference](https://arxiv.org/pdf/2410.03857)
**Summary**: The paper introduces a novel vulnerability called Attack via Implicit Reference (AIR), which exploits context within nested harmless objectives to generate malicious content undetected by current large language models (LLMs). AIR achieves over 90% attack success rates across various state-of-the-art models, including GPT-4, Claude-3.5, and Qwen-2-72B, highlighting the need for improved defense mechanisms against contextual attacks.

### 19. [Still Not Quite There! Evaluating Large Language Models for Comorbid Mental Health Diagnosis](https://arxiv.org/pdf/2410.03908)
**Summary**: The study introduces ANGST, a benchmark for multi-label classification of depression and anxiety comorbidity from social media posts, challenging models to accurately diagnose both conditions simultaneously. Despite GPT-4's superior performance, none of the tested models exceeded an F1 score of 72%, highlighting the limitations of current language models in complex mental health diagnostics.

### 20. [PersonalSum: A User-Subjective Guided Personalized Summarization Dataset for Large Language Models](https://arxiv.org/pdf/2410.03905)
**Summary**: The paper introduces PersonalSum, a novel dataset for personalized summarization, addressing the gap in understanding how individual user preferences differ from generic summaries produced by Large Language Models (LLMs). The dataset includes user profiles, personalized summaries, and machine-generated generic summaries, highlighting that while entities/topics are important, they are not the sole determinants of user preferences, suggesting that personalized summarization remains a complex challenge for LLMs.

### 21. [From Pixels to Personas: Investigating and Modeling Self-Anthropomorphism in Human-Robot Dialogues](https://arxiv.org/pdf/2410.03870)
**Summary**: The paper investigates self-anthropomorphism in human-robot dialogues, analyzing how robots express human-like characteristics and proposing a transition between self-anthropomorphic and non-self-anthropomorphic responses. It introduces the Pix2Persona dataset, which pairs original bot responses with both types of expressions, aiming to develop ethical and engaging AI systems while exploring new categories of bot responses and setting the stage for future research on adjusting anthropomorphism levels dynamically.

### 22. [Can Language Models Reason about Individualistic Human Values and Preferences?](https://arxiv.org/pdf/2410.03868)
**Summary**: The paper introduces IndieValueCatalog, a dataset derived from the World Values Survey, to assess language models' ability to reason about individualistic human values. The study finds that current models struggle with this task, achieving accuracies between 55% to 65%, and highlights the inadequacy of demographic information alone in predicting individual values. The authors propose the Value Inequity Index to measure biases and train Individualistic Value Reasoners to improve model performance, while outlining future research directions for individualistic alignment.

### 23. [KidLM: Advancing Language Models for Children -- Early Insights and Future Directions](https://arxiv.org/pdf/2410.03884)
**Summary**: The paper introduces KidLM, a specialized language model designed for children, which addresses challenges in maintaining linguistic nuances, cognitive needs, and safety standards. It employs a novel data collection pipeline and a training objective called Stratified Masking to enhance the model's performance with child-appropriate content. The model shows improved understanding of lower grade-level text and safety features, offering insights for future research in child-specific language modeling.

### 24. [Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step](https://arxiv.org/pdf/2410.03869)
**Summary**: The paper introduces a novel jailbreaking method called Chain-of-Jailbreak (CoJ) attack, which exploits text-based image generation models by decomposing malicious queries into multiple sub-queries for iterative image editing, thereby bypassing safety measures. The CoJ attack was tested on four image generation services, showing a 60% success rate, significantly higher than other methods. To counter this, the authors propose a defense mechanism, Think Twice Prompting, which effectively mitigates over 95% of CoJ attacks.

### 25. [Structured List-Grounded Question Answering](https://arxiv.org/pdf/2410.03950)
**Summary**: The paper introduces LIST2QA, a new dataset for evaluating question answering systems' ability to utilize structured list information, created from customer service documents using language models and filtering processes. The authors propose an Intermediate Steps for Lists (ISL) approach to enhance model performance, showing significant improvements in metrics like ROUGE-L, correctness, faithfulness, and completeness when compared to baseline models.

### 26. [ActPlan-1K: Benchmarking the Procedural Planning Ability of Visual Language Models in Household Activities](https://arxiv.org/pdf/2410.03907)
**Summary**: The paper introduces ActPlan-1K, a multi-modal planning benchmark for evaluating the procedural planning abilities of visual language models (VLMs) in household activities, incorporating both normal and counterfactual scenarios. The benchmark, based on ChatGPT and iGibson2, includes 1,187 instances with natural language descriptions and environment images, and reveals that current VLMs struggle to generate human-level plans. The authors also propose automatic evaluation metrics using a fine-tuned BLEURT model to aid future research.

### 27. [Grounding Language in Multi-Perspective Referential Communication](https://arxiv.org/pdf/2410.03959)
**Summary**: The paper introduces a task and dataset for generating and comprehending referring expressions in multi-agent environments, where agents must account for differing visual perspectives. It highlights that while automated models perform below human standards, training a speaker model with evidence of communicative success significantly improves performance, surpassing proprietary models.

### 28. [LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity](https://arxiv.org/pdf/2410.03953)
**Summary**: The paper introduces LLM-TOPLA, a novel ensemble method for large language models that maximizes diversity to enhance performance. It features a focal diversity metric, a diversity-optimized pruning algorithm, and a learn-to-ensemble approach to resolve output inconsistencies. Evaluations on four benchmarks demonstrate significant improvements over existing ensemble methods, particularly in accuracy for constrained problems and F1 scores for generative tasks.

### 29. [Question-Answering System for Bangla: Fine-tuning BERT-Bangla for a Closed Domain](https://arxiv.org/pdf/2410.03923)
**Summary**: The paper introduces a question-answering system for Bengali by fine-tuning a BERT-Bangla model on a closed domain dataset sourced from KUET's website. Evaluated with 2500 question-answer pairs, the system achieved an Exact Match score of 55.26% and an F1 score of 74.21%, indicating potential for domain-specific Bengali question-answering but requiring further improvements for complex queries.

### 30. [On the Influence of Gender and Race in Romantic Relationship Prediction from Large Language Models](https://arxiv.org/pdf/2410.03996)
**Summary**: The paper investigates biases in large language models regarding gender and race in romantic relationship prediction. It finds that models are less likely to predict same-gender and interracial relationships, particularly involving Asian names, and that gender is less discernible in Asian names. The study highlights the need for more inclusive and equitable technology development.

### 31. [MetricX-24: The Google Submission to the WMT 2024 Metrics Shared Task](https://arxiv.org/pdf/2410.03983)
**Summary**: The paper introduces MetricX-24, Google's submission to the WMT24 Metrics Shared Task, which is a hybrid reference-based/-free metric designed to score translations without requiring both the source segment and reference. The metric is trained in two stages using DA and MQM ratings, enhanced with synthetic examples to improve robustness against common failure modes. An ablation study shows significant performance gains over its predecessor, MetricX-23, particularly on MQM ratings and a new synthetic challenge set.

### 32. [Take It Easy: Label-Adaptive Self-Rationalization for Fact Verification and Explanation Generation](https://arxiv.org/pdf/2410.04002)
**Summary**: The paper introduces a label-adaptive self-rationalization method for fact verification and explanation generation, extending its application from natural language inference to fact-checking. By fine-tuning models in two steps—first for veracity prediction and then for self-rationalization—the approach significantly improves accuracy on PubHealth and AVeriTec datasets compared to GPT-4. Additionally, the use of synthetic explanations generated by large language models demonstrates the feasibility of low-cost learning, making the method practical for real-world applications.

### 33. [A Simple yet Effective Training-free Prompt-free Approach to Chinese Spelling Correction Based on Large Language Models](https://arxiv.org/pdf/2410.04027)
**Summary**: The paper introduces a novel, training-free, and prompt-free method for Chinese spelling correction using large language models (LLMs). By treating the LLM as a pure language model and employing a minimal distortion model based on character similarities, the approach ensures the corrected sentences remain faithful to the original. The addition of reward strategies tailored to the CSC task further enhances performance, as evidenced by superior results on public datasets compared to existing models.

### 34. [ECon: On the Detection and Resolution of Evidence Conflicts](https://arxiv.org/pdf/2410.04068)
**Summary**: The paper introduces a method to generate and evaluate evidence conflicts in AI-generated content, focusing on detection and resolution by models like NLI, FC, and LLMs. It finds that NLI and LLM models excel in detecting conflicts, especially nuanced ones, but struggle with recall, while FC models falter on lexically similar conflicts. For resolution, LLMs tend to favor one evidence source without clear justification, often relying on prior beliefs.

### 35. [LoRTA: Low Rank Tensor Adaptation of Large Language Models](https://arxiv.org/pdf/2410.04060)
**Summary**: The paper introduces LoRTA, a novel method for Low Rank Tensor Adaptation of large language models, which significantly reduces the number of trainable parameters compared to traditional Low Rank Adaptation (LoRA). By employing low-rank tensor parametrization, LoRTA allows for finer control over adapter size and maintains comparable performance across various benchmarks, demonstrating both efficiency and effectiveness in fine-tuning large language models.

### 36. [Neuron-Level Sequential Editing for Large Language Models](https://arxiv.org/pdf/2410.04045)
**Summary**: The paper introduces Neuron-Level Sequential Editing (NSE), a novel method for continuously updating large language models (LLMs) through multi-round editing without requiring costly retraining. NSE optimizes hidden states using original weights to prevent model failure and iteratively selects neurons for editing to mitigate forgetting, outperforming existing methods in sequential model editing tasks.

### 37. [Large Language Models can Achieve Social Balance](https://arxiv.org/pdf/2410.04054)
**Summary**: The paper explores how large language models (LLMs) can achieve social balance through interactions, finding that the outcome depends on factors like interaction types, peer influence, and the number of simultaneous interactions. The structure and stability of social balance vary across different LLM models and sizes, influenced by their pre-training and alignment processes.

### 38. [SyllableLM: Learning Coarse Semantic Units for Speech Language Models](https://arxiv.org/pdf/2410.04029)
**Summary**: The paper introduces SyllableLM, a novel approach to tokenizing speech data by merging representations into syllable-like units while preserving semantic information. By using a controllable self-supervised technique and a novel distillation method, SyllableLM achieves state-of-the-art performance in syllabic segmentation and clustering, leading to significant improvements in efficiency and performance in speech language modeling tasks.

### 39. [Self-Correction is More than Refinement: A Learning Framework for Visual and Language Reasoning Tasks](https://arxiv.org/pdf/2410.04055)
**Summary**: The paper introduces a Self-Correction Learning (SCL) framework for Vision-Language Models (VLMs) to improve their reasoning abilities through self-generated self-correction data, collected during inference and fine-tuned using Direct Preference Optimization (DPO). The study shows that while VLMs struggle with self-correction during inference, preference fine-tuning significantly enhances their performance, suggesting that self-correction should be viewed as a learning process rather than mere refinement.

### 40. [On Eliciting Syntax from Language Models via Hashing](https://arxiv.org/pdf/2410.04074)
**Summary**: The paper introduces a novel approach to unsupervised parsing by leveraging binary representations to deduce parsing trees from raw text using pre-trained language models. By upgrading the CKY algorithm and employing a contrastive hashing framework with a specialized loss function, the method achieves competitive performance across datasets, demonstrating its effectiveness and efficiency in generating high-quality parsing trees at low computational cost.

### 41. [BloomWise: Enhancing Problem-Solving capabilities of Large Language Models using Bloom's-Taxonomy-Inspired Prompts](https://arxiv.org/pdf/2410.04094)
**Summary**: The paper introduces BloomWise, a prompting technique inspired by Bloom's Taxonomy to enhance Large Language Models' (LLMs) problem-solving capabilities in mathematical and reasoning tasks. By guiding LLMs to start with simple cognitive skills and progressively move to more complex ones through self-evaluation, BloomWise improves performance across multiple datasets. The effectiveness of this approach is demonstrated through extensive experiments and ablations.

### 42. [PsFuture: A Pseudo-Future-based Zero-Shot Adaptive Policy for Simultaneous Machine Translation](https://arxiv.org/pdf/2410.04075)
**Summary**: The paper introduces PsFuture, a zero-shot adaptive read/write policy for Simultaneous Machine Translation (SiMT) that allows models to determine read/write actions without additional training. Additionally, a novel training strategy called Prefix-to-Full (P2F) is proposed to adapt offline translation models for SiMT, leveraging bidirectional attention. Experiments show that PsFuture performs comparably to strong baselines, and P2F further improves performance, achieving a good balance between translation quality and latency.

### 43. [A Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models](https://arxiv.org/pdf/2410.04103)
**Summary**: The paper introduces a novel training paradigm for version updates of Large Language Models (LLMs) that combines the benefits of pre-training from scratch and continual pre-training. By strategically adjusting the learning rate during different stages of training, the proposed method achieves significant cost savings (58% reduction) compared to traditional pre-training from scratch, while maintaining competitive performance. This approach leverages a high initial learning rate and a complete decay process to optimize the update process for LLMs.

### 44. [Exploring LLM-based Data Annotation Strategies for Medical Dialogue Preference Alignment](https://arxiv.org/pdf/2410.04112)
**Summary**: The paper explores using Reinforcement Learning from AI Feedback (RLAIF) to enhance healthcare dialogue models by addressing challenges in preference-aligned data annotation without heavy reliance on medical experts. It introduces a new evaluation framework based on standardized patient examinations and finds that an agent-based approach using Constitutional AI and flowcharts for expressing physician preferences outperforms existing methods in various tests.

### 45. [PAD: Personalized Alignment at Decoding-Time](https://arxiv.org/pdf/2410.04070)
**Summary**: The paper introduces Personalized Alignment at Decoding-time (PAD), a novel framework that aligns Large Language Model (LLM) outputs with diverse personalized preferences during inference without requiring additional training. PAD uses a personalized reward modeling strategy to generate token-level rewards that guide the decoding process, enabling the model to adapt to various user preferences in real-time. Experimental results show that PAD outperforms traditional training-based methods in aligning with diverse preferences and demonstrates generalizability and scalability across different base models.

### 46. [GlobeSumm: A Challenging Benchmark Towards Unifying Multi-lingual, Cross-lingual and Multi-document News Summarization](https://arxiv.org/pdf/2410.04087)
**Summary**: The paper introduces GlobeSumm, a comprehensive benchmark for Multi-lingual, Cross-lingual, and Multi-document Summarization (MCMS), addressing the complexities of real-world news summarization. The authors create a dataset by collecting and restructuring multilingual news reports into an event-centric format and use protocol-guided prompting for reference annotation. GlobeSumm aims to challenge current models by incorporating issues like conflicts, redundancies, and omissions, thereby advancing research in multilingual summarization and the evaluation of large language models.

### 47. [From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression](https://arxiv.org/pdf/2410.04139)
**Summary**: The paper introduces Reading To Compressing (R2C), a novel prompt compression method that leverages the Fusion-in-Decoder (FiD) architecture to identify and retain essential information from long prompts, addressing challenges of global context capture and effective training. R2C improves large language model performance by 6% in out-of-domain evaluations while reducing prompt length by 80%, demonstrating its ability to maintain semantic consistency and computational efficiency.

### 48. [Can the Variation of Model Weights be used as a Criterion for Self-Paced Multilingual NMT?](https://arxiv.org/pdf/2410.04147)
**Summary**: The paper introduces a novel algorithm for selecting minibatch languages in many-to-one neural machine translation (NMT) systems, based on the variation of model weights as measured by smoothed KL divergence. The algorithm outperforms alternating monolingual batches but falls short of shuffled batches in terms of translation quality and convergence speed.

### 49. [Reasoning with Natural Language Explanations](https://arxiv.org/pdf/2410.04148)
**Summary**: The paper explores the role of natural language explanations in enhancing reasoning within Natural Language Inference (NLI) models. It highlights the challenges and opportunities in developing explanation-based NLI systems, emphasizing the need to integrate both material and formal inference aspects. The tutorial provides a foundational understanding of the epistemological and linguistic aspects of explanations, guiding the design and evaluation of NLI models capable of complex reasoning.

### 50. [Consistent Autoformalization for Constructing Mathematical Libraries](https://arxiv.org/pdf/2410.04194)
**Summary**: The paper introduces a method to enhance autoformalization, the process of converting natural language mathematical content into formal language expressions, by using a combination of most-similar retrieval augmented generation (MS-RAG), denoising steps, and auto-correction with syntax error feedback (Auto-SEF). These mechanisms improve the consistency and reliability of autoformalization, particularly in complex and specialized domains, and have been shown to work effectively across various large language models (LLMs).

### 51. [LongGenBench: Long-context Generation Benchmark](https://arxiv.org/pdf/2410.04199)
**Summary**: The paper introduces LongGenBench, a synthetic benchmark designed to evaluate the long-context generation capabilities of Large Language Models (LLMs), addressing the lack of such benchmarks in current retrieval-based tests. The study finds that both API-accessed and open-source models show performance degradation in long-context generation, with varying trends across different model series, highlighting the need for specialized benchmarks in this area.

### 52. [Toxic Subword Pruning for Dialogue Response Generation on Large Language Models](https://arxiv.org/pdf/2410.04155)
**Summary**: The paper introduces ToxPrune, a novel algorithm that prunes toxic subwords from Byte Pair Encoding (BPE) in trained Large Language Models (LLMs) to prevent the generation of toxic content. Contrary to previous findings that pruning BPE tokens can harm machine translation tasks, the authors demonstrate that ToxPrune effectively reduces toxicity in dialogue response generation and even improves the diversity of responses in models like NSFW-3B and Llama-3.1-6B.

### 53. [Overview of Factify5WQA: Fact Verification through 5W Question-Answering](https://arxiv.org/pdf/2410.04236)
**Summary**: The paper introduces Factify5WQA, a shared task focused on automated fake news detection through aspect-based question answering. It provides a dataset where each claim and its supporting document are associated with 5W questions to facilitate comparison. The task evaluates submissions based on BLEU score and classification accuracy, with the best model achieving 69.56% accuracy, significantly outperforming the baseline.

### 54. [RoQLlama: A Lightweight Romanian Adapted Language Model](https://arxiv.org/pdf/2410.04269)
**Summary**: The paper introduces RoQLlama-7b, a quantized version of the Llama2 model adapted for Romanian tasks, demonstrating improved performance on seven Romanian downstream tasks in zero-shot and few-shot setups. Additionally, the authors contribute RoMedQA, a novel Romanian dataset for medical question answering.

### 55. [DiDOTS: Knowledge Distillation from Large-Language-Models for Dementia Obfuscation in Transcribed Speech](https://arxiv.org/pdf/2410.04188)
**Summary**: The paper introduces DiDOTS, a method that leverages Large-Language-Models (LLMs) to obfuscate dementia indicators in speech transcripts, addressing privacy concerns without relying on large labeled datasets. DiDOTS uses knowledge distillation to create a more efficient model with significantly fewer parameters, achieving better privacy performance and utility preservation compared to existing methods.

### 56. [CS4: Measuring the Creativity of Large Language Models Automatically by Controlling the Number of Story-Writing Constraints](https://arxiv.org/pdf/2410.04197)
**Summary**: The paper introduces CS4, a benchmark dataset designed to measure the creativity of large language models (LLMs) in story writing by controlling the number of constraints in prompts. By increasing prompt specificity, CS4 prevents LLMs from retelling familiar stories, allowing for an indirect assessment of creativity. Experiments on various LLMs reveal significant differences in performance under different constraints, with Learning from Human Feedback showing limited impact on enhancing creative output.

### 57. [Persona Knowledge-Aligned Prompt Tuning Method for Online Debate](https://arxiv.org/pdf/2410.04239)
**Summary**: The paper introduces a novel framework that leverages ChatGPT's capabilities to simulate audience personas for enhancing argument quality assessment in online debates. By integrating audience-specific knowledge into smaller language models through prompt tuning, the proposed method significantly improves performance over existing architectures, marking the first exploration of combining argument persuasiveness with audience personae.

### 58. [Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations](https://arxiv.org/pdf/2410.04241)
**Summary**: The paper introduces a novel framework for adaptive Question Answering (QA) that addresses knowledge conflicts by integrating source citations in ambiguous settings. It presents five new datasets, a multi-hop QA dataset, two evaluation metrics, and several baselines to facilitate research in this area, aiming to enhance the trustworthiness and interpretability of QA systems.

### 59. [Correlation-Aware Select and Merge Attention for Efficient Fine-Tuning and Context Length Extension](https://arxiv.org/pdf/2410.04211)
**Summary**: The paper introduces a novel attention architecture that efficiently extends context lengths in large language models by using correlation-aware selection and merging mechanisms, reducing computational resources and fine-tuning time. The proposed method, including a new data augmentation technique with positional encodings, enables models like Llama2-7B and Mistral-7B to handle context lengths up to 1M with significant resource savings and competitive performance.

### 60. [AI as Humanity's Salieri: Quantifying Linguistic Creativity of Language Models via Systematic Attribution of Machine Text against Web Text](https://arxiv.org/pdf/2410.04265)
**Summary**: The paper introduces the CREATIVITY INDEX, a metric to quantify the linguistic creativity of texts by comparing them to existing web content, suggesting that AI's creativity may be largely derived from human-written material. The study finds that human authors, particularly distinguished ones like Hemingway, exhibit higher creativity scores than AI models, and the index outperforms existing systems in detecting machine-generated text.

### 61. [Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia](https://arxiv.org/pdf/2410.04254)
**Summary**: The paper introduces the task of entity insertion in information networks, focusing on Wikipedia, and addresses the challenge of locating suitable positions in text to insert links to target entities. The authors develop a framework called LocEI and its multilingual variant XLocEI, which outperforms baseline models, including GPT-4, and can be applied in a zero-shot manner across languages with minimal performance drop. This work is significant for supporting editors in enhancing the connectivity of Wikipedia across its many language versions.

### 62. [Evaluating Language Model Character Traits](https://arxiv.org/pdf/2410.04272)
**Summary**: The paper introduces a formal framework for evaluating character traits in language models, such as truthfulness and harmfulness, by examining consistent patterns of behavior. It finds that these traits can be stationary or reflective depending on the context and interaction, and their consistency varies with model size, fine-tuning, and prompting. The approach allows for precise characterization of LM behavior without anthropomorphism.

### 63. [Constructing Cloze Questions Generatively](https://arxiv.org/pdf/2410.04266)
**Summary**: The paper introduces CQG, a generative method for constructing cloze questions from articles using neural networks and WordNet, focusing on multigram distractors. The approach involves sense disambiguation, text-to-text transformation, and leveraging WordNet's synset taxonomies to generate and rank distractors, resulting in significantly better performance compared to state-of-the-art methods, as confirmed by human judges.

### 64. [Mechanistic Behavior Editing of Language Models](https://arxiv.org/pdf/2410.04277)
**Summary**: The paper introduces TaRot, a method for task adaptation in large language models (LLMs) that uses learnable rotation matrices optimized via Bayesian Optimization to improve performance on classification and generation tasks. TaRot enhances both zero-shot and few-shot performance, with average improvements of 23.81% and 11.15% respectively across various models and tasks.

### 65. [Calibrating Expressions of Certainty](https://arxiv.org/pdf/2410.04315)
**Summary**: The paper introduces a new method for calibrating linguistic expressions of certainty by modeling uncertainty as distributions over the simplex, which more accurately captures the semantics of phrases like "Maybe" and "Likely." The authors generalize existing measures of miscalibration and propose a novel post-hoc calibration technique to analyze and improve the calibration of both human experts and computational models.

### 66. [ReTok: Replacing Tokenizer to Enhance Representation Efficiency in Large Language Model](https://arxiv.org/pdf/2410.04335)
**Summary**: The paper introduces ReTok, a method to enhance the efficiency of large language models by replacing tokenizers. By reinitializing the input and output layers with the original model's parameters and training them while keeping other parameters fixed, the approach maintains model performance while significantly boosting decoding speed for long texts.

### 67. [CiMaTe: Citation Count Prediction Effectively Leveraging the Main Text](https://arxiv.org/pdf/2410.04404)
**Summary**: The paper introduces CiMaTe, a BERT-based model for predicting future citation counts by effectively utilizing the main text of academic papers. By capturing the sectional structure of papers, CiMaTe outperforms previous methods in citation count prediction, achieving significant improvements in Spearman's rank correlation coefficient across computational linguistics and biology domains.

### 68. [Ordinal Preference Optimization: Aligning Human Preferences via NDCG](https://arxiv.org/pdf/2410.04346)
**Summary**: The paper introduces Ordinal Preference Optimization (OPO), a novel listwise approach that leverages the Normalized Discounted Cumulative Gain (NDCG) to better utilize ordinal rankings of multiple responses in aligning Large Language Models (LLMs) with human preferences. OPO outperforms existing pairwise and listwise methods in aligning multi-response datasets and demonstrates improved performance with an increased pool of negative samples.

### 69. [TIS-DPO: Token-level Importance Sampling for Direct Preference Optimization With Estimated Weights](https://arxiv.org/pdf/2410.04350)
**Summary**: The paper introduces TIS-DPO, a novel approach to Direct Preference Optimization (DPO) that addresses the limitations of treating entire responses as single units by incorporating token-level importance sampling. The authors propose estimating token importance weights using contrastive LLMs and demonstrate that TIS-DPO significantly improves performance on alignment and summarization tasks, outperforming existing methods.

### 70. [Inference Scaling for Long-Context Retrieval Augmented Generation](https://arxiv.org/pdf/2410.04343)
**Summary**: The paper investigates how scaling inference computation in retrieval augmented generation (RAG) can enhance the performance of long-context large language models (LLMs) by focusing on in-context learning and iterative prompting. The study finds that optimal allocation of inference computation leads to nearly linear improvements in RAG performance, and develops a model to predict the best inference parameters for given computation budgets, achieving up to 58.9% gains on benchmark datasets compared to standard RAG.

### 71. [CopyLens: Dynamically Flagging Copyrighted Sub-Dataset Contributions to LLM Outputs](https://arxiv.org/pdf/2410.04454)
**Summary**: The paper introduces CopyLens, a framework designed to dynamically flag copyrighted sub-dataset contributions in Large Language Model (LLM) outputs. By employing a two-stage approach involving token representation fusion and a lightweight LSTM-based network, CopyLens aims to bridge the gap in current copyright detection methods. The framework demonstrates significant improvements in efficiency and accuracy, outperforming existing baselines in experiments.

### 72. [SWEb: A Large Web Dataset for the Scandinavian Languages](https://arxiv.org/pdf/2410.04456)
**Summary**: The paper introduces SWEb, the largest pretraining dataset for Scandinavian languages, containing over one trillion tokens. It describes the dataset's collection and processing pipeline, including a novel model-based text extractor, and presents a new benchmark for evaluating Swedish language models. Models trained on SWEb show competitive performance compared to those trained on FineWeb.

### 73. [Hyper-multi-step: The Truth Behind Difficult Long-context Tasks](https://arxiv.org/pdf/2410.04422)
**Summary**: The paper investigates the challenges faced by long-context language models (LCLMs) in completing difficult long-context tasks, identifying two primary issues: "multi-matching retrieval" and "logic-based retrieval." These issues, which require simultaneous retrieval and logical judgment, are found to be hyper-multi-step in nature, exceeding the capabilities of current LCLMs and explaining their struggles with advanced tasks.

### 74. [Lens: Rethinking Multilingual Enhancement for Large Language Models](https://arxiv.org/pdf/2410.04407)
**Summary**: The paper introduces Lens, a novel method to enhance the multilingual capabilities of large language models (LLMs) by manipulating their internal language representation spaces. By drawing target languages closer to a central language in the language-agnostic subspace and pushing them apart in the language-specific subspace, Lens improves multilingual performance without degrading the central language's capabilities, outperforming existing post-training methods with less computational resources.

### 75. [MindScope: Exploring cognitive biases in large language models through Multi-Agent Systems](https://arxiv.org/pdf/2410.04452)
**Summary**: The paper introduces MindScope, a comprehensive dataset designed to explore cognitive biases in large language models (LLMs) by combining static open-ended questions and a dynamic multi-agent communication framework. The proposed multi-agent detection method, integrating Retrieval-Augmented Generation, competitive debate, and reinforcement learning, significantly enhances bias detection accuracy, outperforming GPT-4 by 35.10%.

### 76. [DAdEE: Unsupervised Domain Adaptation in Early Exit PLMs](https://arxiv.org/pdf/2410.04424)
**Summary**: The paper introduces DAdEE, an unsupervised domain adaptation framework for early exit Pre-trained Language Models (PLMs) that addresses the issue of domain sensitivity in exit classifiers. By employing multi-level adaptation through knowledge distillation and GAN-based adversarial adaptation, DAdEE reduces the domain gap and enhances inference speed while improving domain adaptation performance across various tasks.

### 77. [Revisiting In-context Learning Inference Circuit in Large Language Models](https://arxiv.org/pdf/2410.04468)
**Summary**: The paper introduces a comprehensive circuit to model the inference dynamics of In-context Learning (ICL) in large language models, breaking down the process into three major operations: summarization, semantics merging, and feature retrieval/copy. This circuit effectively captures various ICL phenomena and demonstrates that disabling its components significantly impairs ICL performance, indicating its critical role. The paper also identifies parallel bypass mechanisms that contribute to ICL task solving.

### 78. [Wrong-of-Thought: An Integrated Reasoning Framework with Multi-Perspective Verification and Wrong Information](https://arxiv.org/pdf/2410.04463)
**Summary**: The paper introduces Wrong-of-Thought (WoT), an integrated reasoning framework designed to improve the performance of Large Language Models (LLMs) by addressing two key issues: reliance on single verification methods and ignorance of wrong information. WoT incorporates multi-perspective verification to refine reasoning processes and utilizes wrong information to prevent recurring errors. Experimental results show that WoT outperforms existing methods across multiple datasets and LLMs, particularly in challenging computation tasks.

### 79. [Collapsed Language Models Promote Fairness](https://arxiv.org/pdf/2410.04472)
**Summary**: The paper investigates the phenomenon of Neural Collapse in language models to understand and improve fairness. It finds that debiased models exhibit a collapsed alignment between token representations and word embeddings, which leads to the development of a fine-tuning method that enhances fairness across various debiasing techniques while maintaining model performance on standard tasks.

### 80. [Fine-Grained Prediction of Reading Comprehension from Eye Movements](https://arxiv.org/pdf/2410.04484)
**Summary**: The paper investigates whether reading comprehension can be predicted from eye movements during reading, focusing on the fine-grained task of predicting comprehension at the level of individual questions over a passage. The authors introduce three new multimodal language models and compare them with existing models, evaluating their performance across different reading conditions and participant groups. The results indicate that eye movements provide valuable signals for predicting reading comprehension, despite the complexity of the task.

### 81. [RevMUX: Data Multiplexing with Reversible Adapters for Efficient LLM Batch Inference](https://arxiv.org/pdf/2410.04519)
**Summary**: The paper introduces RevMUX, a novel data multiplexing framework for efficient batch inference in large language models (LLMs). By incorporating reversible adapters in the multiplexer, RevMUX allows for the efficient merging and separation of multiple inputs, enabling higher throughput without significant performance degradation. Experiments across various datasets and LLM backbones show that RevMUX enhances inference efficiency while maintaining classification accuracy.

### 82. [How Does the Disclosure of AI Assistance Affect the Perceptions of Writing?](https://arxiv.org/pdf/2410.04545)
**Summary**: The study investigates how disclosing the use of AI assistance in writing affects perceptions of writing quality. Results indicate that such disclosure, particularly when AI contributes to content generation, lowers average quality ratings and increases variability in individual evaluations. Factors like writing confidence and familiarity with AI tools moderate these effects, and AI-assisted writings are less likely to be ranked highly.

### 83. [ErrorRadar: Benchmarking Complex Mathematical Reasoning of Multimodal Large Language Models Via Error Detection](https://arxiv.org/pdf/2410.04509)
**Summary**: The paper introduces ErrorRadar, a novel benchmark for evaluating the complex mathematical reasoning capabilities of Multimodal Large Language Models (MLLMs) through error detection tasks. ErrorRadar assesses models' abilities to identify and categorize errors in mathematical problem-solving, using a dataset of 2,500 high-quality K-12 problems with detailed annotations. The study reveals that even the best-performing MLLMs, such as GPT-4, lag significantly behind human evaluators in error detection accuracy.

### 84. [LRQ-Fact: LLM-Generated Relevant Questions for Multimodal Fact-Checking](https://arxiv.org/pdf/2410.04616)
**Summary**: The paper introduces LRQ-Fact, an automated framework for multimodal fact-checking that uses Vision-Language Models and Large Language Models to generate relevant questions and answers for assessing information accuracy. The framework includes a rule-based decision-maker to evaluate the veracity of content, and experiments demonstrate improved detection accuracy for multimodal misinformation across different benchmarks and model backbones.

### 85. [Leveraging Large Language Models for Suicide Detection on Social Media with Limited Labels](https://arxiv.org/pdf/2410.04501)
**Summary**: The paper introduces a novel approach for detecting suicidal content on social media using Large Language Models (LLMs), leveraging pseudo-labels generated by prompting LLMs and fine-tuning techniques. An ensemble model combining Qwen2-72B-Instruct, Llama3-8B, Llama3.1-8B, and Gemma2-9B significantly improves detection accuracy, achieving F1 scores of 0.770 and 0.731 on public and private test sets, respectively. The study highlights the impact of model size on prompting performance, with larger models yielding better results.

### 86. [DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination](https://arxiv.org/pdf/2410.04514)
**Summary**: The paper introduces DAMRO, a training-free strategy to reduce object hallucination in Large Vision-Language Models (LVLMs) by analyzing the attention mechanisms of both the visual encoder and the Large Language Model (LLM) decoder. DAMRO uses the classification token (CLS) of ViT to filter out high-attention outlier tokens in the background, thereby mitigating the influence of redundant information during the decoding stage and significantly reducing hallucination in LVLMs.

### 87. [Upsample or Upweight? Balanced Training on Heavily Imbalanced Datasets](https://arxiv.org/pdf/2410.04579)
**Summary**: The paper investigates the equivalence of upsampling (Temperature Sampling) and upweighting loss (Scalarization) in training language models on heavily imbalanced datasets, particularly in multilingual settings. It finds that while these methods are theoretically equivalent under full gradient descent, they diverge in practice with stochastic gradient descent, where upsampling tends to converge faster but risks overfitting. The paper introduces Cooldown, a strategy that adjusts sampling temperature during training to balance convergence speed and overfitting, demonstrating competitive performance and computational efficiency.

### 88. [Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community Retrieval](https://arxiv.org/pdf/2410.04585)
**Summary**: The paper introduces KARE, a framework that integrates knowledge graph (KG) community-level retrieval with large language model (LLM) reasoning to improve healthcare predictions. KARE constructs a comprehensive multi-source KG and uses hierarchical community detection for precise information retrieval, enhancing prediction accuracy by up to 15% on MIMIC datasets. The framework also leverages LLM reasoning to make clinical predictions more interpretable and trustworthy.

### 89. [Towards Secure Tuning: Mitigating Security Risks Arising from Benign Instruction Fine-Tuning](https://arxiv.org/pdf/2410.04524)
**Summary**: The paper investigates the security risks associated with Instruction Fine-Tuning (IFT) of Large Language Models (LLMs), even when using benign instructions. It introduces a novel IFT strategy called Modular Layer-wise Learning Rate (ML-LR), which differentiates learning rates for robust modules identified through a proxy-guided search algorithm. The strategy significantly mitigates security risks without compromising the usability or expertise of the LLMs.

### 90. [Punctuation Prediction for Polish Texts using Transformers](https://arxiv.org/pdf/2410.04621)
**Summary**: The paper presents a solution for the Poleval 2022 Task 1, focusing on punctuation prediction for Polish texts. The approach employs a single HerBERT model fine-tuned on both competition data and an external dataset, achieving a Weighted F1 score of 71.44.

### 91. [FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question Answering](https://arxiv.org/pdf/2410.04526)
**Summary**: The paper introduces FAMMA, an open-source benchmark for financial multilingual multimodal question answering, designed to assess the capabilities of multimodal large language models (MLLMs) in complex financial reasoning. The benchmark includes 1,758 question-answer pairs from university textbooks and exams, covering 8 major finance subfields, with questions in English, Chinese, and French, and a mix of text and images. Despite evaluating advanced models like GPT-4o and Claude-35-Sonnet, FAMMA reveals significant challenges, with only 42% accuracy achieved. The study also explores reasoning chains to improve model performance, highlighting the need for further research in financial QA.

### 92. [ProtocoLLM: Automatic Evaluation Framework of LLMs on Domain-Specific Scientific Protocol Formulation Tasks](https://arxiv.org/pdf/2410.04601)
**Summary**: The paper introduces ProtocoLLM, an automatic evaluation framework for assessing the capabilities of Large Language Models (LLMs) in generating scientific protocols. By using GPT-4 to generate pseudocode as a baseline and Llama-3 as an evaluator, the framework evaluates various LLMs, finding that GPT and Cohere excel in scientific protocol formulation. Additionally, the authors present BIOPROT 2.0, a dataset to support LLM evaluation and training in this domain.

### 93. [Evaluation of Code LLMs on Geospatial Code Generation](https://arxiv.org/pdf/2410.04617)
**Summary**: The paper introduces a benchmark for evaluating Large Language Models (LLMs) on geospatial code generation tasks, addressing the unique challenges posed by this domain. The authors created a dataset of manually curated coding problems that test spatial reasoning, data processing, and tool usage, along with test scenarios for automated code validation. They also tested existing LLMs and made the dataset and evaluation code publicly available, aiming to facilitate the development of more accurate geospatial coding assistants.

### 94. [Control Large Language Models via Divide and Conquer](https://arxiv.org/pdf/2410.04628)
**Summary**: The paper examines the challenges of controlling large language models (LLMs) through prompt-based methods for Lexically Constrained Generation (LCG), identifying issues such as position bias, low responsiveness to decoding parameters, and difficulty with complex constraints. To address these limitations, the authors propose a Divide and Conquer Generation strategy, which significantly improves LLM performance in LCG tasks, achieving over 90% success rate in the most challenging scenarios. This strategy offers a promising approach for enhancing LLM control in more sophisticated text generation applications.

### 95. [Passage Retrieval of Polish Texts Using OKAPI BM25 and an Ensemble of Cross Encoders](https://arxiv.org/pdf/2410.04620)
**Summary**: The paper introduces a winning solution for the Poleval 2023 Task 3: Passage Retrieval challenge, which involves retrieving passages from Polish texts across trivia, legal, and customer support domains. The approach combines the OKAPI BM25 algorithm for document retrieval with an ensemble of multilingual Cross Encoders for reranking. Fine-tuning the reranker models improved performance in the trivia domain but led to worse results in the other domains.

### 96. [Contrastive Learning to Improve Retrieval for Real-world Fact Checking](https://arxiv.org/pdf/2410.04657)
**Summary**: The paper introduces Contrastive Fact-Checking Reranker (CFR), a novel retriever designed to improve evidence retrieval for complex fact-checking tasks by leveraging contrastive learning and fine-tuning on the AVeriTeC dataset. CFR demonstrates a 6% improvement in veracity classification accuracy on the AVeriTeC dataset and shows transferable gains to other datasets, highlighting its effectiveness in enhancing retrieval for fact-checking.

### 97. [Adversarial Multi-Agent Evaluation of Large Language Models through Iterative Debates](https://arxiv.org/pdf/2410.04663)
**Summary**: The paper introduces a novel framework for evaluating large language models (LLMs) by treating them as advocates in a multi-agent system, where they defend their outputs through iterative debates judged by other LLMs. This approach aims to provide a more dynamic and comprehensive evaluation compared to traditional methods, and the authors present a probabilistic model to measure error reduction in such systems, along with experiments to validate their effectiveness.

### 98. [The LLM Effect: Are Humans Truly Using LLMs, or Are They Being Influenced By Them Instead?](https://arxiv.org/pdf/2410.04699)
**Summary**: The paper examines the impact of Large Language Models (LLMs) on human analysis in specialized tasks, finding that while LLMs can significantly speed up task completion, they may introduce anchoring bias that affects the depth and nuance of the analysis. This raises concerns about the trade-off between efficiency and the risk of biased outcomes.

### 99. [MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning in LLMs](https://arxiv.org/pdf/2410.04698)
**Summary**: The paper introduces MathHay, a benchmark designed to evaluate the long-context mathematical reasoning abilities of large language models (LLMs). Unlike previous benchmarks that focus on information retrieval, MathHay requires models to perform complex mathematical reasoning over extended contexts. The study reveals that even top-performing LLMs like Gemini-1.5-Pro-002 struggle with this task, achieving only 51.26% accuracy at 128K tokens, indicating substantial room for improvement in LLM capabilities.

### 100. [Efficient transformer with reinforced position embedding for language models](https://arxiv.org/pdf/2410.04731)
**Summary**: The paper introduces an efficient transformer architecture that enhances performance by reinforcing positional embedding, achieving superior results with fewer encoder-decoder layers. By concatenating positional encoding with trainable token embeddings and normalizing the token embedding matrix, the method significantly reduces training and validation losses, and training time, outperforming a baseline model with the same embedding dimension across multiple translation tasks.

### 101. [$\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction Diversity on Generalization](https://arxiv.org/pdf/2410.04717)
**Summary**: The paper investigates the impact of instruction diversity on the generalization capabilities of large language models (LLMs). It finds that generalization only emerges when training data spans multiple semantic domains, and that cross-domain diversification, even with limited data, significantly enhances a model's adaptability. The study emphasizes the importance of strategic data diversification for both specialist and generalist models, suggesting that expanding beyond core domains and mixing diverse data improves performance.

### 102. [Rule-based Data Selection for Large Language Models](https://arxiv.org/pdf/2410.04715)
**Summary**: The paper introduces a novel rule-based framework for selecting high-quality training data for large language models (LLMs), using the orthogonality of score vectors from diverse rules as a metric. The framework employs an automated pipeline that generates and evaluates rules, selects the most independent ones using the determinantal point process (DPP), and applies them to select data for LLM training. Experimental results show that this DPP-based method outperforms other data selection approaches in terms of rating precision and model performance across various tasks.

### 103. [TableRAG: Million-Token Table Understanding with Language Models](https://arxiv.org/pdf/2410.04739)
**Summary**: The paper introduces TableRAG, a Retrieval-Augmented Generation framework designed to improve language models' ability to understand large tables by efficiently retrieving and encoding crucial information, thereby reducing prompt lengths and enhancing scalability. The authors developed new benchmarks from Arcade and BIRD-SQL datasets to evaluate TableRAG, showing it achieves state-of-the-art performance in large-scale table understanding.

### 104. [Formality is Favored: Unraveling the Learning Preferences of Large Language Models on Data with Conflicting Knowledge](https://arxiv.org/pdf/2410.04784)
**Summary**: The study investigates how large language models (LLMs) handle conflicting information in training data, finding that they prefer formal texts and those with fewer spelling errors, leading to faster learning and better retention of knowledge. This preference is consistent across models and languages, with larger models showing stronger tendencies, suggesting that LLMs trust data that aligns with the majority and can be influenced by manipulating consistency with the majority data.

### 105. [DAPE V2: Process Attention Score as Feature Map for Length Extrapolation](https://arxiv.org/pdf/2410.04798)
**Summary**: The paper introduces a novel approach to improve Transformer models by treating attention scores as feature maps and applying convolution operations to enhance their expressiveness. This method addresses the limitations of the traditional key-query dot product in handling length extrapolation, leading to significant performance improvements in Transformers.

### 106. [Forgetting Curve: A Reliable Method for Evaluating Memorization Capability for Long-context Models](https://arxiv.org/pdf/2410.04727)
**Summary**: The paper identifies limitations in current methods for evaluating the memorization capabilities of long-context language models and introduces a new approach called the "forgetting curve." This method is robust across different corpora and experimental settings, does not rely on prompts, and can be applied to models of any size. The study applies the forgetting curve to various models, providing insights into the effectiveness of transformer extensions and questioning the effective memorization lengths of RNN/SSM-based models.

### 107. [Document-level Causal Relation Extraction with Knowledge-guided Binary Question Answering](https://arxiv.org/pdf/2410.04752)
**Summary**: The paper introduces a Knowledge-guided binary Question Answering (KnowQA) method for Event-Event Causal Relation Extraction (ECRE), addressing challenges like lack of document-level modeling and causal hallucinations. The proposed method, involving Event Structure Construction and Binary Question Answering, achieves state-of-the-art performance on the MECI dataset and shows high generalizability and low inconsistency, especially with complete event structures post-fine-tuning.

### 108. [GARLIC: LLM-Guided Dynamic Progress Control with Hierarchical Weighted Graph for Long Document QA](https://arxiv.org/pdf/2410.04790)
**Summary**: The paper introduces GARLIC, a novel retrieval method for long document QA that uses a Hierarchical Weighted Directed Acyclic Graph and LLM-guided dynamic progress control to outperform existing RAG methods and even Llama 3.1 in accuracy, while maintaining computational efficiency. The method leverages attention mechanisms and allows for multi-path retrieval, enabling dynamic adjustment of information depth based on query complexity.

### 109. [LPZero: Language Model Zero-cost Proxy Search from Zero](https://arxiv.org/pdf/2410.04808)
**Summary**: The paper introduces LPZero, a novel framework for automatically designing Zero-cost (ZC) proxies in Neural Architecture Search (NAS) that significantly reduces computational demands. Unlike existing ZC proxies that rely on expert knowledge, LPZero uses genetic programming to find optimal symbolic compositions, achieving higher ranking consistency and outperforming current approaches on NLP tasks.

### 110. [Representing the Under-Represented: Cultural and Core Capability Benchmarks for Developing Thai Large Language Models](https://arxiv.org/pdf/2410.04795)
**Summary**: The paper introduces two new benchmarks, Thai-H6 and Thai Cultural and Linguistic Intelligence Benchmark (ThaiCLI), to address the lack of evaluation frameworks for Thai large language models (LLMs). These benchmarks aim to enhance both the core capabilities and cultural understanding of Thai LLMs, providing a comprehensive evaluation tool for researchers and developers. The datasets and evaluation code will be made publicly available to support further research in this area.

### 111. [MINER: Mining the Underlying Pattern of Modality-Specific Neurons in Multimodal Large Language Models](https://arxiv.org/pdf/2410.04819)
**Summary**: The paper introduces MINER, a framework for identifying modality-specific neurons (MSNs) in multimodal large language models (MLLMs), addressing the lack of explainability in these models. The framework consists of four stages and demonstrates that deactivating a small percentage of MSNs significantly impacts model performance, indicating the importance of these neurons in processing multimodal data.

### 112. [Rationale-Aware Answer Verification by Pairwise Self-Evaluation](https://arxiv.org/pdf/2410.04838)
**Summary**: The paper introduces REPS, a method for improving answer verification by focusing on the validity of rationales in addition to the correctness of final answers. REPS uses pairwise self-evaluation to select valid rationales from candidates generated by large language models, leading to verifiers that outperform conventional methods on reasoning benchmarks. The study highlights the importance of rationale validity in training reliable verifiers for complex reasoning tasks.

### 113. [As Simple as Fine-tuning: LLM Alignment via Bidirectional Negative Feedback Loss](https://arxiv.org/pdf/2410.04834)
**Summary**: The paper introduces a novel loss function called Bidirectional Negative Feedback (BNF) for aligning large language models (LLMs), which addresses the instability and hyperparameter sensitivity issues of Direct Preference Optimization (DPO). BNF simplifies the alignment process by eliminating the need for pairwise contrastive losses and extra hyperparameters, achieving comparable performance on QA benchmarks and better balance between value alignment and reasoning ability on reasoning benchmarks.

### 114. [Activation Scaling for Steering and Interpreting Language Models](https://arxiv.org/pdf/2410.04962)
**Summary**: The paper explores the concept of steering language models by scaling activation vectors to correct incorrect predictions, such as flipping "Rome is in France" to "Rome is in Italy." The authors propose a three-term objective to ensure interventions are effective, faithful, and minimal. They demonstrate that activation scaling is more interpretable and efficient than steering vectors, allowing for precise modifications to the model's internal workings.

### 115. [Leveraging Grammar Induction for Language Understanding and Generation](https://arxiv.org/pdf/2410.04878)
**Summary**: The paper introduces an unsupervised grammar induction method for enhancing language understanding and generation tasks. By constructing a grammar parser that induces constituency structures and dependency relations, the authors integrate these features into the Transformer model as a syntactic mask. This approach outperforms the original Transformer and other models with external parsers across various tasks, demonstrating the effectiveness of explicitly modeling grammatical structures in neural networks.

### 116. [Intent Classification for Bank Chatbots through LLM Fine-Tuning](https://arxiv.org/pdf/2410.04925)
**Summary**: The study investigates the effectiveness of fine-tuning SlovakBERT for intent classification in bank chatbots, comparing it to multilingual generative models like Llama 8b instruct and Gemma 7b instruct. Results show that SlovakBERT achieves superior in-scope accuracy and lower out-of-scope false positive rates, making it the preferred model for this application.

### 117. [SkillMatch: Evaluating Self-supervised Learning of Skill Relatedness](https://arxiv.org/pdf/2410.05006)
**Summary**: The paper introduces SkillMatch, a benchmark for evaluating skill relatedness in human resources, constructed from expert knowledge mined from millions of job ads. It also proposes a self-supervised learning approach using Sentence-BERT adapted for skill co-occurrence, significantly outperforming traditional models. The release of SkillMatch aims to advance research in skill-based recommendation systems.

### 118. [Named Clinical Entity Recognition Benchmark](https://arxiv.org/pdf/2410.05046)
**Summary**: The paper introduces a Named Clinical Entity Recognition Benchmark, a standardized platform for evaluating language models in healthcare. It uses curated clinical datasets and assesses models' ability to identify and classify entities like diseases and medications, with performance measured by F1-score. The benchmark aims to promote transparency and innovation in clinical NLP by ensuring consistency and interoperability across healthcare systems.

### 119. [A test suite of prompt injection attacks for LLM-based machine translation](https://arxiv.org/pdf/2410.05047)
**Summary**: The paper introduces a comprehensive test suite for evaluating prompt injection attacks (PIAs) on LLM-based machine translation systems, extending previous work by Sun and Miceli-Barone. The suite covers all language pairs in the WMT 2024 General Machine Translation task and includes various attack formats to assess the robustness of these systems against malicious inputs that subvert intended translations.

### 120. [ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering](https://arxiv.org/pdf/2410.05077)
**Summary**: The paper introduces ZEBRA, a zero-shot question answering framework that enhances commonsense reasoning by combining retrieval, case-based reasoning, and introspection without requiring additional training of the language model. ZEBRA outperforms existing methods and strong language models across multiple benchmarks, achieving an average accuracy improvement of up to 4.5 points.

### 121. [Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes](https://arxiv.org/pdf/2410.05052)
**Summary**: The paper introduces a novel technique called weight scaling as reparameterization (WeSaR) to mitigate loss spikes in large language models by addressing the non-uniformity of parameter norms. WeSaR stabilizes training by uniformly scaling the norms of the original parameters through the introduction of gate parameters, leading to improved performance and faster convergence compared to other initialization methods.

### 122. [ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery](https://arxiv.org/pdf/2410.05080)
**Summary**: The paper introduces ScienceAgentBench, a benchmark designed to rigorously assess the capabilities of language agents in automating data-driven scientific discovery. It includes 102 tasks from 44 peer-reviewed publications across four disciplines, validated by experts, and evaluates the performance of five LLMs using various frameworks. The results show that even the best-performing agents can only solve a fraction of the tasks independently, highlighting the current limitations of language agents in fully automating scientific workflows.

### 123. [Explanation sensitivity to the randomness of large language models: the case of journalistic text classification](https://arxiv.org/pdf/2410.05085)
**Summary**: The paper investigates the impact of random elements in training large language models (LLMs) on the explainability of their predictions, focusing on French journalistic text classification. It finds that different random seeds yield models with similar accuracy but varying explanations, suggesting the need to characterize the statistical distribution of explanations. The study also explores a simpler model that offers stable explanations but lower accuracy, and demonstrates that incorporating features from LLM explanations can improve this model, highlighting a trade-off between accuracy and explainability.

### 124. [Investigating large language models for their competence in extracting grammatically sound sentences from transcribed noisy utterances](https://arxiv.org/pdf/2410.05099)
**Summary**: The study investigates whether large language models (LLMs) can effectively extract grammatically sound sentences from noisy transcribed utterances, akin to human cognitive abilities. Experiments in Polish reveal that while LLMs can extract some well-structured utterances, many are incorrect, suggesting either incomplete acquisition or ineffective application of syntactic-semantic rules. The findings indicate that LLMs' comprehension of noisy speech remains less proficient compared to human capabilities.

### 125. [SparsePO: Controlling Preference Alignment of LLMs via Sparse Token Masks](https://arxiv.org/pdf/2410.05102)
**Summary**: The paper introduces SparsePO, a novel approach to preference optimization (PO) for language models that focuses on weighting tokens differently based on their relevance to human preferences. By learning sparse weight masks, SparsePO allows the model to prioritize certain tokens during training, leading to improved performance in tasks such as sentiment control, dialogue, summarization, and text-to-code generation. The method demonstrates better alignment with human preferences and enhances reasoning capabilities compared to existing PO methods.

### 126. [ReasoningRank: Teaching Student Models to Rank through Reasoning-Based Knowledge Distillation](https://arxiv.org/pdf/2410.05168)
**Summary**: The paper introduces ReasoningRank, a reranking approach that enhances transparency by generating explicit and comparison reasoning to explain document rankings. It uses large language models to create these explanations and distills this knowledge into smaller, more efficient student models, which perform competitively while reducing computational demands.

### 127. [Deciphering the Interplay of Parametric and Non-parametric Memory in Retrieval-augmented Language Models](https://arxiv.org/pdf/2410.05162)
**Summary**: The study investigates how Retrieval-Augmented Generation (RAG) models like \textsc{Atlas} balance parametric (internal) and non-parametric (retrieved) memory during information processing. Through causal mediation analysis, it reveals that the model prioritizes retrieved context over internal knowledge when both are available, and identifies two key mechanisms: determining context relevance and computing output representations for copying relevant information.

### 128. [Enhancing Equity in Large Language Models for Medical Applications](https://arxiv.org/pdf/2410.05180)
**Summary**: The paper discusses the potential of large language models (LLMs) in medical applications but highlights significant inequities affecting specific racial, gender, and underrepresented groups. To address these disparities, the authors introduce EquityGuard, a framework designed to detect and mitigate biases in LLM-based medical applications, thereby promoting equity across diverse populations.

### 129. [CTC-GMM: CTC guided modality matching for fast and accurate streaming speech translation](https://arxiv.org/pdf/2410.05146)
**Summary**: The paper introduces CTC-GMM, a method that enhances streaming speech translation (ST) by using Connectionist Temporal Classification (CTC) to align speech and text sequences, allowing the use of machine translation (MT) data to improve ST models. Evaluations on FLEURS and CoVoST2 datasets show significant improvements in translation accuracy and decoding speed.

### 130. [RevisEval: Improving LLM-as-a-Judge via Response-Adapted References](https://arxiv.org/pdf/2410.05193)
**Summary**: The paper introduces RevisEval, a new evaluation method for text generation that uses large language models (LLMs) to create response-adapted references, which are revised versions of the original responses. This approach aims to improve the reliability of LLM-as-a-Judge evaluations by enhancing the relevance of references, and it demonstrates superior performance across various natural language generation tasks compared to traditional evaluation methods.

### 131. [Studying and Mitigating Biases in Sign Language Understanding Models](https://arxiv.org/pdf/2410.05206)
**Summary**: The paper investigates biases in sign language understanding models trained on crowd-sourced datasets like ASL Citizen, highlighting potential inequities. It applies bias mitigation techniques during model training, finding that these methods reduce performance disparities without compromising accuracy. The study also releases demographic data from the ASL Citizen dataset to support future efforts in bias mitigation.

### 132. [Beyond Correlation: Interpretable Evaluation of Machine Translation Metrics](https://arxiv.org/pdf/2410.05183)
**Summary**: The paper introduces an interpretable evaluation framework for machine translation (MT) metrics, moving beyond traditional correlation with human judgments to assess metrics using Precision, Recall, and F-score. This approach provides clearer insights into metric performance, particularly for new use cases like data filtering and translation re-ranking. The study also highlights issues with the reliability of manually curated data, finding low agreement between Direct Assessments+Scalar Quality Metrics (DA+SQM) and Multidimensional Quality Metrics (MQM) annotations.

### 133. [SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe](https://arxiv.org/pdf/2410.05248)
**Summary**: The paper introduces SFTMix, a novel approach to improve instruction-tuning in large language models (LLMs) by leveraging Mixup-based regularization to address uneven confidence levels in the semantic representation space. This method enhances performance across various tasks without requiring high-quality curated datasets, demonstrating scalability and adaptability to different LLM families and dataset sizes.

### 134. [Cookbook: A framework for improving LLM generative abilities via programmatic data generating templates](https://arxiv.org/pdf/2410.05224)
**Summary**: The paper introduces Cookbook, a framework for programmatically generating training data to improve large language models (LLMs) without relying on human or LLM-generated data, thereby avoiding privacy and legal issues. Cookbook uses simple patterns over random tokens to create datasets that enhance LLM performance on specific tasks, achieving up to a 52.7% accuracy improvement. The framework also optimizes multi-task performance by mixing data from various templates, leading to superior results on the GPT4ALL evaluation suite.

### 135. [CasiMedicos-Arg: A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures](https://arxiv.org/pdf/2410.05235)
**Summary**: The paper introduces CasiMedicos-Arg, the first multilingual dataset for Medical Question Answering, which includes both correct and incorrect diagnoses along with natural language explanations annotated with argumentative structures by medical professionals. The dataset, comprising 558 clinical cases in four languages, is enriched with argument components and relations, facilitating the development of AI tools to aid in medical education and decision-making explanation.

### 136. [Causal Micro-Narratives](https://arxiv.org/pdf/2410.05252)
**Summary**: The paper introduces a method for classifying causal micro-narratives from text using a subject-specific ontology of causes and effects. Applied to inflation narratives, the approach leverages a human-annotated dataset and evaluates several large language models, with the fine-tuned Llama 3.1 8B achieving high F1 scores. The research highlights linguistic ambiguity as a significant challenge and suggests the framework has broad applications in social science.

### 137. [GLEE: A Unified Framework and Benchmark for Language-based Economic Environments](https://arxiv.org/pdf/2410.05254)
**Summary**: The paper introduces GLEE, a unified framework and benchmark for studying the behavior of Large Language Models (LLMs) in language-based economic environments. It defines standardized games to evaluate LLMs' rationality, human-like behavior, and outcomes in terms of efficiency and fairness, providing a comprehensive tool for comparing LLM agents to human players and analyzing their performance in various economic contexts.

### 138. [Differential Transformer](https://arxiv.org/pdf/2410.05258)
**Summary**: The paper introduces Diff Transformer, a novel architecture that enhances attention mechanisms by amplifying relevant context while reducing noise through a differential attention mechanism. This approach outperforms traditional Transformers in various language modeling tasks and shows significant improvements in practical applications like long-context modeling, key information retrieval, and hallucination mitigation. Diff Transformer also enhances robustness in in-context learning and reduces activation outliers, positioning it as a promising advancement in large language models.

### 139. [Grounding Partially-Defined Events in Multimodal Data](https://arxiv.org/pdf/2410.05267)
**Summary**: The paper introduces a multimodal approach to grounding partially-defined events in video and text data, addressing the challenges of understanding complex events from unstructured video snippets. It proposes a three-stage span retrieval task and a benchmark, MultiVENT-G, with densely annotated videos and text documents, to evaluate LLM-driven methods for multimodal event analysis. The results highlight the difficulties in abstract event understanding and show potential for improving event-centric video-language systems.

### 140. [Data Advisor: Dynamic Data Curation for Safety Alignment of Large Language Models](https://arxiv.org/pdf/2410.05269)
**Summary**: The paper introduces Data Advisor, a method that enhances large language models (LLMs) by dynamically curating data to improve safety alignment. By monitoring and analyzing the generated data, Data Advisor identifies deficiencies and guides subsequent data generation to address these issues, thereby improving both data quality and coverage. Experiments show that Data Advisor effectively enhances model safety across multiple LLMs without compromising their utility.

### 141. [TurtleBench: Evaluating Top Language Models via Real-World Yes/No Puzzles](https://arxiv.org/pdf/2410.05262)
**Summary**: The paper introduces TurtleBench, a novel evaluation benchmark for Large Language Models (LLMs) that uses real-world user guesses from the Turtle Soup Puzzle platform to assess logical reasoning capabilities. Unlike static datasets, TurtleBench provides a dynamic and user-centric approach, enhancing the reliability of evaluations. The study finds that OpenAI's o1 series models did not perform exceptionally well, suggesting potential limitations in their reasoning strategies and the need for further research on Chain-of-Thought techniques.

### 142. [Getting in the Door: Streamlining Intake in Civil Legal Services with Large Language Models](https://arxiv.org/pdf/2410.03762)
**Summary**: The paper explores the use of large language models (LLMs) to streamline the legal intake process for civil legal services, aiming to reduce the time and resources required to determine eligibility. By integrating logical rules with LLMs, the authors develop a digital platform that provides eligibility recommendations, achieving an F1 score of .82 with the best model, thereby potentially aiding in closing the access to justice gap.

### 143. [Efficient Streaming LLM for Speech Recognition](https://arxiv.org/pdf/2410.03752)
**Summary**: The paper introduces SpeechLLM-XL, a decoder-only model designed for efficient streaming speech recognition, addressing the limitations of existing methods by processing audio in configurable chunks with reduced computational cost. The model achieves competitive word error rates (WER) on LibriSpeech datasets and maintains performance on long-form utterances significantly longer than those used during training.

### 144. [SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models](https://arxiv.org/pdf/2410.03750)
**Summary**: The paper introduces SQFT, a method for low-precision sparse fine-tuning of large pre-trained models (LPMs) that enables efficient adaptation in resource-constrained environments. SQFT allows for the merging of sparse weights with low-rank adapters while maintaining sparsity and accuracy, addressing challenges related to different numerical precisions. The approach is validated across various scenarios and sparsity levels, demonstrating its effectiveness.

### 145. [FutureFill: Fast Generation from Convolutional Sequence Models](https://arxiv.org/pdf/2410.03766)
**Summary**: The paper introduces FutureFill, a method for accelerating auto-regressive generation in sequence prediction models using convolutional operators. It reduces generation time from linear to square root relative to context length and requires a smaller cache size compared to standard models. Experimental results confirm the method's correctness and efficiency.

### 146. [Metadata Matters for Time Series: Informative Forecasting with Transformers](https://arxiv.org/pdf/2410.03806)
**Summary**: The paper introduces MetaTST, a novel approach that integrates metadata into Transformer-based time series forecasting models to enhance accuracy and interpretability. By converting unstructured metadata into structured text and encoding it with large language models, MetaTST enriches the embedding process and improves forecasting performance across diverse scenarios, achieving state-of-the-art results in both short- and long-term forecasting benchmarks.

### 147. [Can Mamba Always Enjoy the "Free Lunch"?](https://arxiv.org/pdf/2410.03810)
**Summary**: The paper investigates the theoretical limitations of Mamba, a model known for its constant-size overhead during inference, in comparison to Transformers. It finds that while Mamba performs well in many sequence modeling tasks, it may face bottlenecks in tasks requiring the COPY operation and in solving dynamic programming (DP) problems, especially when the sequence length scales. The study concludes that Mamba's efficiency gains are context-dependent and not universally applicable, thus questioning the notion of a "free lunch" in all scenarios.

### 148. [Harnessing Task Overload for Scalable Jailbreak Attacks on Large Language Models](https://arxiv.org/pdf/2410.04190)
**Summary**: The paper introduces a scalable jailbreak attack on Large Language Models (LLMs) that exploits resource constraints to bypass safety mechanisms. By engaging the LLM in a computationally intensive preliminary task, the attack saturates the model's processing capacity, preventing the activation of safety protocols when executing the target instruction. This method demonstrates high success rates across various LLMs and emphasizes the need for more robust safety measures that consider resource limitations.

### 149. [Variational Language Concepts for Interpreting Foundation Language Models](https://arxiv.org/pdf/2410.03964)
**Summary**: The paper introduces a new approach called VAriational Language Concept (VALC) to enhance the interpretability of Foundation Language Models (FLMs) by moving beyond word-level interpretations to concept-level interpretations. The authors propose a variational Bayesian framework that optimizes language concepts to better explain FLM predictions, demonstrating its effectiveness through empirical results on various datasets.

### 150. [Enhancing Future Link Prediction in Quantum Computing Semantic Networks through LLM-Initiated Node Features](https://arxiv.org/pdf/2410.04251)
**Summary**: The paper explores enhancing link prediction in quantum computing semantic networks by initializing node features using Large Language Models (LLMs). This approach leverages LLMs to generate rich node descriptions, reducing manual feature creation and improving the performance of link prediction models in graph neural networks, as evidenced by evaluations on a quantum computing dataset.

### 151. [Learning Code Preference via Synthetic Evolution](https://arxiv.org/pdf/2410.03837)
**Summary**: The paper introduces CodeFavor, a framework for training pairwise code preference models using synthetic evolution data, and CodePrefBench, a benchmark for evaluating code preferences across correctness, efficiency, and security. The evaluation demonstrates that CodeFavor significantly improves model accuracy and cost-effectiveness compared to larger models, while highlighting the limitations and costs of human-based code preference assessments.

### 152. [DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories Search](https://arxiv.org/pdf/2410.03864)
**Summary**: The paper introduces DOTS, a method that enables large language models (LLMs) to dynamically reason by searching for optimal reasoning trajectories tailored to each question and the LLM's capabilities. The approach involves defining atomic reasoning actions, searching for the best action sequences for training questions, and using these sequences to train the LLM to plan reasoning for new questions. Experiments demonstrate that DOTS outperforms static reasoning methods and improves LLMs' ability to adapt reasoning depth based on problem complexity.

### 153. [Fundamental Limitations on Subquadratic Alternatives to Transformers](https://arxiv.org/pdf/2410.04271)
**Summary**: The paper demonstrates that any subquadratic alternative to the Transformer architecture, such as heuristic algorithms or models like Mamba, cannot perform certain critical tasks, particularly document similarity tasks, as effectively as Transformers. This implies that for tasks involving document similarity, the quadratic time complexity of Transformers cannot be avoided.

### 154. [Hyperbolic Fine-tuning for Large Language Models](https://arxiv.org/pdf/2410.04010)
**Summary**: The paper investigates the suitability of Euclidean space for embedding tokens in large language models (LLMs) and finds that token embeddings exhibit a high degree of hyperbolicity, suggesting a tree-like structure. To leverage this, the authors propose HypLoRA, a method for fine-tuning LLMs in hyperbolic space, which significantly improves performance on complex reasoning tasks, as evidenced by a 13.0% improvement on the AQuA dataset.

### 155. [Improving Arabic Multi-Label Emotion Classification using Stacked Embeddings and Hybrid Loss Function](https://arxiv.org/pdf/2410.03979)
**Summary**: The paper introduces a novel approach to improve Arabic multi-label emotion classification by combining stacked embeddings from fine-tuned language models (ArabicBERT, MarBERT, and AraBERT), a meta-learner, and a hybrid loss function. The hybrid loss function, which includes class weighting, label correlation matrix, and contrastive learning, effectively addresses class imbalances and enhances the model's performance, particularly in predicting minority emotions. The study demonstrates significant improvements in key metrics and presents a generalizable framework for low-resource emotion classification tasks.

### 156. [Latent Feature Mining for Predictive Model Enhancement with Large Language Models](https://arxiv.org/pdf/2410.04347)
**Summary**: The paper introduces FLAME, a framework that uses large language models to infer latent features from text data, thereby enhancing predictive models in domains with limited and ethically challenging data collection. The framework is validated in the criminal justice and healthcare domains, showing that the inferred latent features improve the performance of downstream classifiers.

### 157. [Language Model-Driven Data Pruning Enables Efficient Active Learning](https://arxiv.org/pdf/2410.04275)
**Summary**: The paper introduces ActivePrune, a novel data pruning strategy for active learning that uses language models to reduce computational costs by efficiently pruning the unlabeled data pool. It employs a two-stage pruning process and a perplexity reweighting method to enhance diversity, outperforming existing methods in terms of both selection quality and efficiency, with up to 74% reduction in end-to-end active learning time.

### 158. [TUBench: Benchmarking Large Vision-Language Models on Trustworthiness with Unanswerable Questions](https://arxiv.org/pdf/2410.04107)
**Summary**: The paper introduces TUBench, a benchmark designed to evaluate the reliability of Large Vision-Language Models (LVLMs) using unanswerable questions, addressing the underexplored issue of hallucination in scenarios where visual information is insufficient. The benchmark includes questions crafted from diverse domains to test various reasoning abilities, and it shows that even top-performing models like Gemini-1.5-Pro and GPT-4o struggle with determining answerability, highlighting the need for improved trustworthiness in LVLMs.

### 159. [OD-Stega: LLM-Based Near-Imperceptible Steganography via Optimized Distributions](https://arxiv.org/pdf/2410.04328)
**Summary**: The paper introduces OD-Stega, a method for near-imperceptible steganography using Large Language Models (LLMs) to generate stego-texts with minimal token usage. It optimizes the entropy of token replacement probabilities to ensure natural language fluency while embedding secret messages. The approach addresses tokenization mismatches and combines optimized distributions with vocabulary truncation and sequence-level heuristics for enhanced efficiency.

### 160. [Algorithmic Capabilities of Random Transformers](https://arxiv.org/pdf/2410.04368)
**Summary**: The paper investigates the algorithmic capabilities of randomly initialized transformers, focusing on tasks that can be learned by optimizing only the embedding layers. It finds that these random transformers can perform a variety of meaningful algorithmic tasks, suggesting that some capabilities are inherent in the architecture before training begins.

### 161. [Suspiciousness of Adversarial Texts to Human](https://arxiv.org/pdf/2410.04377)
**Summary**: The paper investigates the concept of human suspiciousness in adversarial texts, distinguishing it from the imperceptibility focus in image-based adversarial examples. It introduces a novel dataset of human evaluations on the suspiciousness of adversarial sentences and develops a regression-based model to quantify and reduce this suspiciousness, aiming to create more effective adversarial texts that are less detectable by humans.

### 162. [Realizing Video Summarization from the Path of Language-based Semantic Understanding](https://arxiv.org/pdf/2410.04511)
**Summary**: The paper introduces a novel video summarization framework that leverages the strengths of multiple Video-based Large Language Models (VideoLLMs) without requiring fine-tuning, inspired by the Mixture of Experts (MoE) paradigm. This approach generates comprehensive and coherent textual summaries by integrating visual and audio content, enhancing semantic understanding and performance in downstream tasks like summary video generation.

### 163. [Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning](https://arxiv.org/pdf/2410.04691)
**Summary**: The paper challenges the conventional belief that fine-tuning outperforms in-context learning (ICL) with sufficient training data, finding that ICL excels in capturing implicit patterns in tasks. Through experiments on specialized datasets, ICL demonstrated superior accuracy and pattern recognition compared to fine-tuning, even with significantly fewer training samples. The authors propose circuit shift theory to explain ICL's effectiveness.

### 164. [Learning How Hard to Think: Input-Adaptive Allocation of LM Computation](https://arxiv.org/pdf/2410.04707)
**Summary**: The paper introduces a method for adaptively allocating computational resources during language model decoding, based on predicting the difficulty of generating high-quality outputs for different inputs. By dynamically adjusting the amount of computation used, the approach reduces computational costs by up to 50% without compromising output quality or improves quality by up to 10% within a fixed budget, across various tasks like programming, mathematics, and dialog.

### 165. [Can LLMs plan paths with extra hints from solvers?](https://arxiv.org/pdf/2410.05045)
**Summary**: The paper investigates how Large Language Models (LLMs) can be enhanced in solving robotic planning tasks by incorporating feedback from solvers. It explores various feedback strategies and evaluates the performance of different LLMs on a range of planning problems. The study finds that solver-generated feedback improves LLM performance on moderately difficult problems but struggles with harder ones, highlighting the models' limitations in long-term planning and higher-order reasoning.

### 166. [ImProver: Agent-Based Automated Proof Optimization](https://arxiv.org/pdf/2410.04753)
**Summary**: The paper introduces ImProver, an agent-based system using large language models to optimize formal proofs in Lean by rewriting them to meet user-defined criteria such as length, readability, or modularity. ImProver incorporates improvements like symbolic Lean context and error-correction techniques, demonstrating significant enhancements in proof quality across various mathematical domains.

### 167. [DEPT: Decoupled Embeddings for Pre-training Language Models](https://arxiv.org/pdf/2410.05021)
**Summary**: The paper introduces DEPT, a novel pre-training framework that decouples embedding layers from the transformer model, allowing for more efficient and effective training across heterogeneous data sources. DEPT reduces parameter count and communication costs, enhances model generalization, and enables custom vocabularies per data source, demonstrated through a 1.3 billion-parameter model pre-training across diverse languages.

### 168. [Regressing the Relative Future: Efficient Policy Optimization for Multi-turn RLHF](https://arxiv.org/pdf/2410.04612)
**Summary**: The paper introduces REFUEL, an efficient policy optimization approach for multi-turn reinforcement learning from human feedback (RLHF) in large language models (LLMs). REFUEL addresses the covariate shift issue by using a single model to estimate Q-values and training on self-generated data, framing the problem as a sequence of regression tasks. Empirical results show that REFUEL outperforms state-of-the-art methods and enables smaller models to outperform larger ones in long multi-turn dialogues.

### 169. [TLDR: Token-Level Detective Reward Model for Large Vision Language Models](https://arxiv.org/pdf/2410.04734)
**Summary**: The paper introduces TLDR, a Token-Level Detective Reward Model designed to provide fine-grained annotations for each text token in multimodal language models, addressing the limitations of existing binary feedback systems. TLDR uses a perturbation-based method to generate synthetic hard negatives and their token-level labels, enhancing model performance and speeding up human annotation processes.

### 170. [Intriguing Properties of Large Language and Vision Models](https://arxiv.org/pdf/2410.04751)
**Summary**: The paper investigates the intriguing properties of large language and vision models (LLVMs), highlighting their surprising performance on advanced reasoning tasks despite relatively low performance on fundamental perception tasks. Through extensive experiments across 10 benchmarks, the study reveals that LLVMs process images globally, can solve math problems without detailed numerical perception, and that cross-modal alignment is overfitted to complex reasoning tasks, potentially compromising original perceptual capabilities. The findings suggest future directions for improving LLVMs and creating more challenging evaluation benchmarks.

### 171. [Efficient Inference for Large Language Model-based Generative Recommendation](https://arxiv.org/pdf/2410.05165)
**Summary**: The paper introduces AtSpeed, an alignment framework designed to accelerate Large Language Model (LLM)-based generative recommendation by improving top-K sequence alignment between the draft model and the target LLM, and by relaxing the verification strategy to reduce unnecessary LLM calls. Empirical results show significant speedups, with near 2x improvement under strict top-K verification and up to 2.5x under relaxed sampling verification.

### 172. [TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention](https://arxiv.org/pdf/2410.05076)
**Summary**: The paper introduces TidalDecode, an algorithm and system designed to improve the decoding efficiency of large language models (LLMs) by using position persistent sparse attention. By leveraging spatial coherence in token selection and incorporating a few layers with full attention to identify key tokens, TidalDecode reduces the overhead of token selection in sparse attention without compromising the quality of generated text. The evaluation demonstrates that TidalDecode achieves up to a 2.1x reduction in decoding latency while maintaining performance comparable to full attention methods.

### 173. [Precise Model Benchmarking with Only a Few Observations](https://arxiv.org/pdf/2410.05222)
**Summary**: The paper introduces an empirical Bayes (EB) estimator to improve the precision of large language model (LLM) performance estimates on specific topics within a dataset, especially when sample sizes are small. By balancing direct and regression estimates, the EB approach consistently reduces mean squared error and provides more reliable confidence intervals compared to traditional methods.

### 174. [VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks](https://arxiv.org/pdf/2410.05160)
**Summary**: The paper introduces VLM2Vec, a contrastive training framework that transforms vision-language models into universal multimodal embedding models capable of handling a wide range of tasks. The authors also present MMEB, a comprehensive benchmark for evaluating multimodal embeddings across 36 datasets. Evaluation results demonstrate that VLM2Vec significantly outperforms existing models, achieving an average improvement of 10% to 20% on both in-distribution and out-of-distribution datasets.

### 175. [Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape Perspective](https://arxiv.org/pdf/2410.05192)
**Summary**: The paper introduces the Warmup-Stable-Decay (WSD) learning rate schedule, which allows for indefinite training without a fixed compute budget by maintaining a constant learning rate and then rapidly decaying it when needed. The authors propose a "river valley" loss landscape to explain the observed behavior, where a high stable learning rate drives rapid progress along a river, while a fast decay phase moves the model closer to optimal performance. This approach outperforms traditional schedules in obtaining strong language model checkpoints across various compute budgets.

### 176. [Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic Compositionality](https://arxiv.org/pdf/2410.05210)
**Summary**: The paper introduces Fine-grained Selective Calibrated CLIP (FSC-CLIP) to enhance compositional understanding in pre-trained vision and language models without degrading multi-modal performance. By integrating local hard negative loss and selective calibrated regularization, FSC-CLIP maintains fine-grained negative supervision while preserving representational integrity, achieving state-of-the-art compositionality and strong multi-modal capabilities across benchmarks.

### 177. [Density estimation with LLMs: a geometric investigation of in-context learning trajectories](https://arxiv.org/pdf/2410.05218)
**Summary**: The paper investigates how large language models (LLMs) like LLaMA-2 estimate probability density functions (PDFs) through in-context learning, using Intensive Principal Component Analysis (InPCA) to visualize their learning trajectories. It finds that LLMs follow distinct trajectories compared to traditional methods like histograms and Gaussian KDE, suggesting an adaptive kernel width and shape in their density estimation process. This study provides insights into the probabilistic reasoning mechanisms of LLMs.

### 178. [Paraphrase Identification with Deep Learning: A Review of Datasets and Methods](https://arxiv.org/pdf/2212.06933)
**Summary**: The paper reviews current methods and datasets for paraphrase identification, highlighting the challenges posed by the inconsistent representation of paraphrase types in training data. It introduces a refined typology (ReParaphrased) to address these disparities and suggests new research directions to improve AI-based plagiarism detection.

### 179. [PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs](https://arxiv.org/pdf/2410.05265)
**Summary**: The paper introduces PrefixQuant, a novel technique that isolates high-frequency outlier tokens offline to enable efficient per-tensor static quantization in Large Language Models (LLMs), outperforming traditional per-token dynamic quantization methods. PrefixQuant achieves significant improvements in perplexity and accuracy, with faster inference speeds, demonstrating its effectiveness in enhancing both performance and efficiency.

### 180. [TuneVLSeg: Prompt Tuning Benchmark for Vision-Language Segmentation Models](https://arxiv.org/pdf/2410.05239)
**Summary**: The paper introduces TuneVLSeg, an open-source benchmarking framework for evaluating prompt tuning techniques in Vision-Language Segmentation Models (VLSMs) across diverse domains. The study tests six prompt tuning strategies on eight medical and natural domain datasets, finding that visual prompt tuning is effective and less complex than multimodal approaches, especially under significant domain shifts.

### 181. [Diversity Over Size: On the Effect of Sample and Topic Sizes for Topic-Dependent Argument Mining Datasets](https://arxiv.org/pdf/2205.11472)
**Summary**: The paper investigates the impact of dataset composition on Argument Mining performance in few- and zero-shot settings, finding that fine-tuning is crucial but reducing training sample size by up to 90% can still achieve 95% of maximum performance. The findings are consistent across multiple tasks and datasets, and a new dataset is introduced for future benchmarking.

### 182. [Prompts have evil twins](https://arxiv.org/pdf/2311.07064)
**Summary**: The paper introduces "evil twins," unintelligible prompts that elicit similar behavior in language models as their natural-language counterparts, despite being humanly incomprehensible. These prompts are shown to transfer across different models and are generated by solving a maximum-likelihood problem, highlighting potential applications in prompt engineering and model behavior analysis.

### 183. [Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents](https://arxiv.org/pdf/2410.05243)
**Summary**: The paper introduces UGround, a universal visual grounding model for GUI agents, which operates by directly interpreting GUI elements visually rather than relying on text-based representations like HTML. By leveraging a large dataset of 10M GUI elements and their referring expressions, UGround significantly outperforms existing models, enabling GUI agents to navigate digital interfaces more effectively, akin to human perception. This approach demonstrates the potential for more robust and versatile GUI agents in real-world applications.

### 184. [Social Bias Probing: Fairness Benchmarking for Language Models](https://arxiv.org/pdf/2311.09090)
**Summary**: The paper introduces a new framework for probing social biases in language models, focusing on disparate treatment across diverse demographic groups. It introduces SoFa, a large-scale benchmark that reveals more nuanced biases than previously recognized, particularly highlighting the pronounced disparate treatment of identities associated with different religions. The findings suggest that language models mirror real-life adversities faced by various groups, including women and people with disabilities.

### 185. [Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question Answering (Published in Findings of EMNLP 2024)](https://arxiv.org/pdf/2309.02233)
**Summary**: The paper introduces LLM-AMT, a system that augments black-box large language models (LLMs) with medical textbooks to enhance their performance in biomedical question answering. By integrating plug-and-play modules like Query Augmenter, Hybrid Textbook Retriever, and Knowledge Self-Refiner, LLM-AMT significantly improves response accuracy, outperforming specialized models like Med-PaLM 2 and demonstrating that medical textbooks are more effective than Wikipedia as a knowledge source in the medical domain.

### 186. [When "A Helpful Assistant" Is Not Really Helpful: Personas in System Prompts Do Not Improve Performances of Large Language Models](https://arxiv.org/pdf/2311.10054)
**Summary**: The study investigates the impact of personas in system prompts on the performance of Large Language Models (LLMs), finding that adding personas does not consistently improve performance across various tasks. However, the gender, type, and domain of the persona can influence prediction accuracy, and aggregating results from the best persona for each question can enhance accuracy, though automatically identifying the best persona remains challenging.

### 187. [Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding](https://arxiv.org/pdf/2312.17044)
**Summary**: The paper surveys methods for enhancing the length extrapolation capabilities of Transformers, focusing on positional encoding (PE) as the primary factor influencing this ability. It categorizes existing approaches into extrapolatable PEs, position interpolation, and randomized position methods, and highlights challenges and future research directions in this area. The survey aims to provide a comprehensive understanding of current techniques and inspire further advancements.

### 188. [PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching](https://arxiv.org/pdf/2312.05621)
**Summary**: The paper introduces PILLOW, a method to enhance the performance of Low-Rank Adaptation (LoRA) in fine-tuning Large Language Models (LLMs) by using a discrimination-based prompting approach. PILLOW leverages in-context learning and a matching network to select prompts from a user-defined pool, significantly reducing computational costs while maintaining comparable performance to traditional instruction fine-tuning methods.

### 189. [Self-Contradictory Reasoning Evaluation and Detection](https://arxiv.org/pdf/2311.09603)
**Summary**: The paper investigates self-contradictory reasoning in large language models (LLMs), finding that these models often produce inconsistent reasoning, particularly in tasks requiring contextual understanding or commonsense. While GPT-4 can detect some self-contradictions, its performance is significantly lower than human ability, highlighting the need for more robust evaluation methods in reasoning tasks beyond mere performance metrics.

### 190. [Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue](https://arxiv.org/pdf/2401.04700)
**Summary**: The paper investigates the trade-offs between improving the factuality of large language models (LLMs) through model editing and maintaining their general abilities in reasoning and other tasks. It finds that current editing methods often degrade the model's overall performance due to excessive weight alterations. To address this, the authors propose RECT, a regularization method that constrains the complexity of weight updates, effectively mitigating side effects while preserving editing effectiveness.

### 191. [Large Language Models for Propaganda Span Annotation](https://arxiv.org/pdf/2311.09812)
**Summary**: The paper explores the use of Large Language Models (LLMs) like GPT-4 for detecting and annotating propagandistic spans in text, addressing the challenge of limited datasets in lower-resourced languages. The study finds that providing more context to GPT-4 in prompts enhances its performance over human annotators and that GPT-4's labels can train specialized models achieving state-of-the-art results on an Arabic test set. This work demonstrates the potential of LLMs in creating annotated datasets for propaganda detection, even with limited human expertise.

### 192. [Multi-User Chat Assistant (MUCA): a Framework Using LLMs to Facilitate Group Conversations](https://arxiv.org/pdf/2401.04883)
**Summary**: The paper introduces the Multi-User Chat Assistant (MUCA), an LLM-based framework designed to facilitate group conversations by addressing the complexities of multi-user interactions through three main modules: Sub-topic Generator, Dialog Analyzer, and Conversational Strategies Arbitrator. MUCA aims to determine "What" to say, "When" to respond, and "Who" to answer, enhancing user engagement in goal-oriented discussions. Additionally, the paper proposes an LLM-based Multi-User Simulator (MUS) to accelerate MUCA's development and optimization.

### 193. [SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully](https://arxiv.org/pdf/2401.05930)
**Summary**: The paper introduces Self-Highlighted Hesitation (SH2), an inference-time method designed to enhance the factual accuracy of large language models (LLMs) by focusing on tokens with lower prediction probabilities. SH2 highlights these potentially more informative tokens and incorporates them into the decoding process, using contrastive decoding to emphasize differences in output probabilities. Experimental results show that SH2 significantly improves the factual accuracy of models like LLaMA-7b, LLaMA2-7b, and Mistral-7b across various hallucination tasks, without requiring additional data or models.

### 194. [Large Language Models are Geographically Biased](https://arxiv.org/pdf/2402.02680)
**Summary**: The paper investigates the geographic biases present in Large Language Models (LLMs), revealing that these models exhibit systemic errors in geospatial predictions, particularly favoring regions with higher socioeconomic conditions. The study introduces a bias score to quantify these biases, highlighting significant variation across different LLMs, and emphasizes the need for understanding and mitigating such biases to ensure fairness and accuracy.

### 195. [R-Judge: Benchmarking Safety Risk Awareness for LLM Agents](https://arxiv.org/pdf/2401.10019)
**Summary**: The paper introduces R-Judge, a benchmark designed to assess the safety risk awareness of large language model (LLM) agents in interactive environments. It includes 569 multi-turn interaction records across 27 risk scenarios and 10 risk types, revealing that current LLMs struggle with identifying safety risks, with the best-performing model achieving only 74.42% accuracy. The study highlights the need for improved risk awareness and demonstrates that fine-tuning on safety judgment tasks significantly enhances model performance.

### 196. [Numerical Claim Detection in Finance: A New Financial Dataset, Weak-Supervision Model, and Market Analysis](https://arxiv.org/pdf/2402.11728)
**Summary**: The paper introduces a new financial dataset for detecting claims in analyst reports and earnings calls, aiming to analyze their impact on market returns. A novel weak-supervision model is proposed, which integrates expert knowledge to outperform existing methods, and is used to create a measure of optimism that correlates with earnings surprises and returns.

### 197. [Corrective Retrieval Augmented Generation](https://arxiv.org/pdf/2401.15884)
**Summary**: The paper introduces Corrective Retrieval Augmented Generation (CRAG), a method designed to enhance the robustness of retrieval-augmented generation (RAG) by incorporating a lightweight retrieval evaluator and a decompose-then-recompose algorithm. CRAG ensures more accurate and relevant document retrieval by assessing the quality of retrieved documents and selectively focusing on key information, leading to improved performance across various generation tasks.

### 198. [Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models](https://arxiv.org/pdf/2402.10353)
**Summary**: The paper introduces a null-input prompting method to calibrate intrinsic bias in pre-trained language models, aiming to improve zero/few-shot learning performance while maintaining computational efficiency. By using GPT-4-generated null-meaning inputs and a distribution disparity loss, the method adjusts bias parameters to create a more equitable probability distribution, leading to significant enhancements in zero/few-shot learning across various datasets.

### 199. [Head-wise Shareable Attention for Large Language Models](https://arxiv.org/pdf/2402.11819)
**Summary**: The paper introduces head-wise shareable attention as a method to reduce the memory footprint of Large Language Models (LLMs) by sharing parameters across attention heads. It proposes two techniques: **DirectShare**, which reuses pre-trained weights without retraining, and **PostShare**, which involves post-training with constraints on weight similarity before sharing. Both methods demonstrate that fine-grained weight sharing can maintain model performance while significantly reducing memory usage.

### 200. [Pedagogical Alignment of Large Language Models](https://arxiv.org/pdf/2402.05000)
**Summary**: The paper explores the use of Learning from Human Preferences (LHP) algorithms to align Large Language Models (LLMs) with effective teaching strategies, termed "pedagogical alignment." By generating synthetic datasets to overcome the scarcity of high-quality preference data, the study demonstrates that LHP methods outperform standard supervised fine-tuning, significantly improving the models' ability to guide students through problem-solving processes. The authors also introduce new perplexity-based metrics to evaluate pedagogical alignment, highlighting the potential of LHP methods in enhancing LLMs' educational effectiveness.

### 201. [Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications](https://arxiv.org/pdf/2402.16367)
**Summary**: The paper investigates the internal neuron activation patterns of large language models (LLMs) when processing different languages by converting dense models into fine-grained Mixture of Experts (MoE) architectures. Through visual analysis and experiments, the study identifies patterns in expert activations across languages, revealing insights into multilingual processing mechanisms. The findings are applied to improve sparse activation and model pruning techniques, demonstrating superior performance compared to random pruning and unpruned models in certain languages.

### 202. [Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning](https://arxiv.org/pdf/2402.13950)
**Summary**: The paper investigates the faithfulness of large language models (LLMs) in using their intermediate reasoning steps to generate final answers, finding that LLMs do not reliably use these steps. To address this, the authors introduce FRODO, a framework that tailors small-sized LMs to generate correct reasoning steps and robustly reason over them, significantly outperforming competitive baselines and improving robustness and generalization.

### 203. [Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese](https://arxiv.org/pdf/2402.17302)
**Summary**: The study explores the capability of Large Language Models (LLMs) to generate culturally relevant commonsense question-answering (QA) datasets for Indonesian and Sundanese languages. By creating datasets through both LLM-based and human-annotated methods, the researchers found that while GPT-4 Turbo can generate questions with adequate general knowledge, it lacks the cultural depth of human-generated content, particularly in lower-resource languages like Sundanese, where fluency errors are more frequent.

### 204. [sDPO: Don't Use Your Data All at Once](https://arxiv.org/pdf/2403.19270)
**Summary**: The paper introduces sDPO, an extension of direct preference optimization (DPO) that aligns large language models (LLMs) with human preferences by using preference datasets in a stepwise manner. This approach allows for more precise alignment and results in a final model that outperforms other LLMs, including those with more parameters.

### 205. [Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems](https://arxiv.org/pdf/2402.17840)
**Summary**: The paper investigates the vulnerability of Retrieval-Augmented Generation (RAG) systems, particularly those using instruction-tuned Language Models (LMs), to datastore leakage through prompt injection. The study reveals that adversaries can exploit these systems to extract verbatim text data, with the risk increasing as model size scales up. The authors demonstrate successful extraction from various LMs and propose mitigation strategies, including position bias elimination, to reduce this vulnerability.

### 206. [Tokenization Is More Than Compression](https://arxiv.org/pdf/2402.18376)
**Summary**: The paper challenges the common belief that fewer tokens lead to better performance in natural language processing tasks by introducing PathPiece, a tokenizer designed to minimize token count. Through extensive experiments, the authors find that fewer tokens do not necessarily improve downstream performance, questioning the understanding of effective tokenization. The study highlights the importance of pre-tokenization and the benefits of using BPE for vocabulary initialization, offering new insights into tokenizer design.

### 207. [Editing Conceptual Knowledge for Large Language Models](https://arxiv.org/pdf/2403.06259)
**Summary**: The paper introduces a novel approach to editing conceptual knowledge in Large Language Models (LLMs) by creating the ConceptEdit benchmark dataset and new evaluation metrics. It finds that while existing editing methods can modify concept-level definitions, they often distort related instance-level knowledge, highlighting the need for improved techniques to balance these changes.

### 208. [HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models](https://arxiv.org/pdf/2403.11456)
**Summary**: The paper introduces HateCOT, a large English dataset with over 52,000 samples, enhanced with GPT-3.5Turbo-generated explanations, aimed at improving the generalization of offensive speech detection models. Pretraining on HateCOT significantly boosts the performance of open-source Large Language Models across multiple benchmark datasets, even in zero-shot and few-shot scenarios, while also enhancing the quality of model explanations.

### 209. [The Generation Gap: Exploring Age Bias in the Value Systems of Large Language Models](https://arxiv.org/pdf/2404.08760)
**Summary**: The study investigates age bias in Large Language Models (LLMs) by comparing their value systems to those of different age groups using the World Value Survey. It finds that LLMs tend to align more with younger demographics, particularly in the US context, but this bias varies across different value categories. The research also examines the effect of including age identity in prompts, revealing challenges in reducing value discrepancies between LLMs and various age cohorts.

### 210. [FAC$^2$E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition](https://arxiv.org/pdf/2403.00126)
**Summary**: The paper introduces FAC$^2$E, a framework for evaluating large language models (LLMs) by distinguishing between language and cognitive capabilities. It breaks down the evaluation process into three sub-steps: recalling knowledge, utilizing knowledge, and solving problems, providing a detailed diagnosis of LLMs' performance. The study identifies a common weakness in knowledge utilization and suggests a knowledge-enhanced method to improve LLM performance.

### 211. [MetaAligner: Towards Generalizable Multi-Objective Alignment of Language Models](https://arxiv.org/pdf/2403.17141)
**Summary**: The paper introduces MetaAligner, a novel approach for multi-objective preference alignment in large language models that is both policy-agnostic and generalizable. It achieves this through a three-stage process: dynamic objectives reformulation, conditional weak-to-strong correction, and a generalizable inference method. MetaAligner significantly reduces training costs and can align with unseen objectives, outperforming existing methods in multi-objective alignment while saving substantial GPU training hours.

### 212. [Evalverse: Unified and Accessible Library for Large Language Model Evaluation](https://arxiv.org/pdf/2404.00943)
**Summary**: The paper introduces Evalverse, a unified and accessible library designed to simplify the evaluation of Large Language Models (LLMs) by consolidating various evaluation tools into a single framework. This tool allows users with minimal AI expertise to request and receive detailed LLM evaluation reports, facilitated through integration with communication platforms like Slack, making it a valuable resource for both researchers and practitioners.

### 213. [Robust Pronoun Fidelity with English LLMs: Are they Reasoning, Repeating, or Just Biased?](https://arxiv.org/pdf/2404.03134)
**Summary**: The paper introduces the task of pronoun fidelity, which measures the ability of language models to correctly reuse pronouns in context, and presents the RUFF dataset for evaluating this task. The study finds that while models generally perform well with pronouns like "he/him/his," they struggle with "she/her/her," singular "they," and neopronouns, and are easily distracted by unrelated pronouns in the text. The results highlight the need for improved robustness and reasoning in pronoun usage by language models.

### 214. [Characterizing LLM Abstention Behavior in Science QA with Context Perturbations](https://arxiv.org/pdf/2404.12452)
**Summary**: The paper investigates the ability of large language models (LLMs) to abstain from answering science questions when provided with insufficient or incorrect context. Through experiments on four QA datasets and six LLMs, the study reveals significant variability in abstention behavior across models and question types, with some models struggling to abstain on boolean questions. The analysis also shows that altering context, such as replacing gold context with irrelevant information, can paradoxically improve both abstention and overall task performance, suggesting the need for changes in QA dataset design and evaluation to better assess model correctness and impact.

### 215. [NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding](https://arxiv.org/pdf/2404.13627)
**Summary**: The paper introduces NegotiationToM, a benchmark designed to evaluate the Theory of Mind (ToM) capabilities of large language models (LLMs) in real-world negotiation scenarios involving complex mental states like desires, beliefs, and intentions. The benchmark, based on the Belief-Desire-Intention (BDI) theory, reveals that current state-of-the-art LLMs struggle significantly compared to humans, even with advanced reasoning techniques like chain-of-thought.

### 216. [SpaceByte: Towards Deleting Tokenization from Large Language Modeling](https://arxiv.org/pdf/2404.14408)
**Summary**: The paper introduces SpaceByte, a byte-level decoder architecture designed to eliminate the need for tokenization in large language models while maintaining performance. By incorporating larger transformer blocks after specific byte sequences like spaces, SpaceByte significantly improves byte-level modeling and achieves performance comparable to tokenized models, demonstrating its effectiveness within a fixed compute budget.

### 217. [Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering](https://arxiv.org/pdf/2404.14741)
**Summary**: The paper introduces Generate-on-Graph (GoG), a training-free method that leverages Large Language Models (LLMs) to address Incomplete Knowledge Graph Question Answering (IKGQA) by generating new factual triples when the provided Knowledge Graph (KG) is insufficient. GoG operates through a Thinking-Searching-Generating framework, treating the LLM as both an agent and a KG, and outperforms previous methods in experimental evaluations on two datasets.

### 218. [Red Teaming Language Models for Processing Contradictory Dialogues](https://arxiv.org/pdf/2405.10128)
**Summary**: The paper introduces a novel task for detecting and modifying contradictory statements in dialogues, inspired by context faithfulness and dialogue comprehension research. It develops a Red Teaming framework that uses a dataset of contradictory dialogues with explanatory labels to improve detection, explanation, and modification of contradictions, highlighting the significance of logical consistency in conversational AI.

### 219. [Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning](https://arxiv.org/pdf/2405.06680)
**Summary**: The paper investigates the compositionality of large language models (LLMs) in mathematical reasoning by introducing logical traps into problem descriptions, creating a dataset called MathTrap. The study finds that LLMs struggle to spontaneously combine mathematical knowledge with logical reasoning to solve these novel cases, but performance can be passively improved through external interventions like prompts and fine-tuning.

### 220. [Student Data Paradox and Curious Case of Single Student-Tutor Model: Regressive Side Effects of Training LLMs for Personalized Learning](https://arxiv.org/pdf/2404.15156)
**Summary**: The paper identifies the "Student Data Paradox," where training Large Language Models (LLMs) on extensive student-tutor dialogue datasets to personalize education leads to a decline in the models' factual knowledge and reasoning abilities. The study demonstrates this paradox through quantitative analysis and introduces "hallucination tokens" as a partial solution, highlighting the ongoing challenge of balancing accurate student behavior modeling with maintaining the LLM's educational integrity.

### 221. [Representation noising effectively prevents harmful fine-tuning on LLMs](https://arxiv.org/pdf/2405.14577)
**Summary**: The paper introduces Representation Noising (RepNoise), a defense mechanism against harmful fine-tuning attacks on large language models (LLMs). RepNoise removes information related to harmful representations, making it difficult for attackers to recover them during fine-tuning, even when they have access to the model weights. The method is shown to be effective across different types of harm and does not impair the model's general capabilities or its ability to perform harmless tasks.

### 222. [Promoting Constructive Deliberation: Reframing for Receptiveness](https://arxiv.org/pdf/2405.15067)
**Summary**: The paper introduces a method to enhance online discussions by automatically reframing disagreeing responses to appear more receptive, based on six identified strategies. Experiments using a Reddit dataset show that these reframed replies are perceived as significantly more receptive than original replies, demonstrating the potential of computational frameworks to align language models with human social constructs for improved content moderation.

### 223. [DAPE: Data-Adaptive Positional Encoding for Length Extrapolation](https://arxiv.org/pdf/2405.14722)
**Summary**: The paper introduces Data-Adaptive Positional Encoding (DAPE), a novel method that dynamically adjusts positional encoding based on input context and learned priors, addressing the limitations of fixed positional encodings in transformers. Experiments on various datasets show that DAPE significantly improves model performance and length generalization, achieving superior results even when extrapolating to much longer sequences than those used during training.

### 224. [Language in Vivo vs. in Silico: Size Matters but Larger Language Models Still Do Not Comprehend Language on a Par with Humans](https://arxiv.org/pdf/2404.14883)
**Summary**: The study examines the performance of three large language models (LLMs) on a grammaticality judgment task, comparing their accuracy and stability to that of humans. While the largest model, ChatGPT-4, outperforms humans in recognizing grammatical sentences, it shows less stability and sensitivity to ungrammaticality compared to humans. The findings suggest that model scaling alone may not fully bridge the gap in language comprehension between LLMs and humans.

### 225. [RLSF: Reinforcement Learning via Symbolic Feedback](https://arxiv.org/pdf/2405.16661)
**Summary**: The paper introduces Reinforcement Learning via Symbolic Feedback (RLSF), a novel fine-tuning method for Large Language Models (LLMs) that leverages reasoning tools to provide detailed, token-level feedback, addressing limitations of traditional RLHF methods. RLSF outperforms traditional approaches across various tasks, demonstrating that smaller LLMs fine-tuned with RLSF can surpass much larger models like GPT-4.

### 226. [Superposed Decoding: Multiple Generations from a Single Autoregressive Inference Pass](https://arxiv.org/pdf/2405.18400)
**Summary**: The paper introduces Superposed Decoding, a novel algorithm that generates multiple text drafts from a single autoregressive inference pass, significantly reducing computational costs compared to traditional methods that require multiple passes. The approach combines and filters drafts using n-gram interpolation, resulting in coherent and factual outputs that are faster and preferred by users in compute-normalized settings.

### 227. [WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models](https://arxiv.org/pdf/2405.14768)
**Summary**: The paper introduces WISE, a novel approach for lifelong model editing in large language models (LLMs) that addresses the challenge of balancing reliability, generalization, and locality. WISE employs a dual parametric memory scheme, with a main memory for pretrained knowledge and a side memory for edited knowledge, along with a router to manage queries. It also features a knowledge-sharding mechanism to prevent conflicts during continual editing. Experiments demonstrate that WISE outperforms existing methods in various LLM architectures, effectively resolving the "impossible triangle" in lifelong model editing scenarios.

### 228. [Evaluating and Safeguarding the Adversarial Robustness of Retrieval-Based In-Context Learning](https://arxiv.org/pdf/2405.15984)
**Summary**: The paper investigates the robustness of Retrieval-Augmented In-Context Learning (ICL) against adversarial attacks, finding that while it improves robustness against test sample attacks, it shows overconfidence in demonstrations, leading to higher attack success rates. The study introduces a training-free defense method, DARD, which enhances robustness by enriching the example pool with attacked samples, achieving a 15% reduction in attack success rate compared to baselines.

### 229. [Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization](https://arxiv.org/pdf/2406.01171)
**Summary**: The paper surveys the evolving use of personas in large language models (LLMs), categorizing current research into two main areas: LLM Role-Playing, where models are assigned specific personas, and LLM Personalization, where models adapt to user personas. It introduces a systematic taxonomy and methods for evaluating LLM personality, aiming to unify and advance the field.

### 230. [Robo-Instruct: Simulator-Augmented Instruction Alignment For Finetuning CodeLLMs](https://arxiv.org/pdf/2405.20179)
**Summary**: The paper introduces ROBO-INSTRUCT, a method that combines a robot simulator (ROBOSIM) with an LLM-aided alignment process (INSTALIGN) to generate diverse and correct programs for fine-tuning Code LLMs. This approach significantly improves the performance of the fine-tuned model, achieving better results than proprietary LLMs like GPT-3.5-Turbo and Gemini-Pro.

### 231. [Auto-Arena: Automating LLM Evaluations with Agent Peer Battles and Committee Discussions](https://arxiv.org/pdf/2405.20267)
**Summary**: The paper introduces Auto-Arena, a framework that automates the evaluation of large language models (LLMs) through agent-based peer battles and committee discussions, eliminating the need for manual human evaluations. The system demonstrates a 92.14% correlation with human preferences in extensive experiments involving 15 LLMs, outperforming existing benchmarks and offering a reliable, automated alternative for LLM evaluation.

### 232. [MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark (Published at NeurIPS 2024 Track Datasets and Benchmarks)](https://arxiv.org/pdf/2406.01574)
**Summary**: The paper introduces MMLU-Pro, an advanced version of the MMLU benchmark, designed to challenge language models with more complex reasoning tasks and a larger choice set. MMLU-Pro significantly reduces model accuracy and demonstrates greater stability under different prompts, making it a more robust and challenging benchmark for evaluating AI language comprehension and reasoning capabilities.

### 233. [ComplexTempQA: A Large-Scale Dataset for Complex Temporal Question Answering](https://arxiv.org/pdf/2406.04866)
**Summary**: The paper introduces ComplexTempQA, a massive dataset with over 100 million question-answer pairs, designed to challenge AI models in temporal question answering. It surpasses existing benchmarks in scale and complexity, featuring questions that require sophisticated reasoning across time, entities, and events, and includes detailed metadata for comprehensive evaluation.

### 234. [WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild](https://arxiv.org/pdf/2406.04770)
**Summary**: The paper introduces WildBench, an automated evaluation framework for benchmarking large language models (LLMs) using challenging, real-world user queries. It employs two metrics, WB-Reward and WB-Score, to systematically evaluate model outputs and provide interpretable results, showing strong correlation with human-voted Elo ratings.

### 235. [Advancing Semantic Textual Similarity Modeling: A Regression Framework with Translated ReLU and Smooth K2 Loss](https://arxiv.org/pdf/2406.05326)
**Summary**: The paper introduces a regression framework for Semantic Textual Similarity (STS) that addresses the limitations of contrastive learning by proposing two new loss functions: Translated ReLU and Smooth K2 Loss. This approach allows for fine-grained similarity modeling and outperforms existing methods on seven STS benchmarks, suggesting potential improvements for contrastive learning models.

### 236. [Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models](https://arxiv.org/pdf/2406.09289)
**Summary**: The paper investigates the mechanisms behind jailbreaking in large language models, finding that a single jailbreak vector can mitigate different types of jailbreaks, suggesting a common internal mechanism. It also identifies a potential commonality in how effective jailbreaks reduce the model's perception of prompt harmfulness, providing insights for developing more robust countermeasures against jailbreaking.

### 237. [FacLens: Transferable Probe for Foreseeing Non-Factuality in Large Language Models](https://arxiv.org/pdf/2406.05328)
**Summary**: The paper introduces FacLens, a lightweight model designed to predict non-factual responses from large language models (LLMs) before they are generated. FacLens leverages hidden representations of questions from various LLMs, demonstrating similar patterns that enhance its transferability and reduce development costs. The model outperforms existing methods in both effectiveness and efficiency.

### 238. [Understanding "Democratization" in NLP and ML Research](https://arxiv.org/pdf/2406.11598)
**Summary**: The paper examines how the term "democratization" is used in NLP and ML research, finding that it often refers to the accessibility of technologies rather than deeper democratic principles. The authors advocate for integrating theories of democratization, such as deliberation and debate, to create more genuinely democratic technologies.

### 239. [Annotation alignment: Comparing LLM and human annotations of conversational safety](https://arxiv.org/pdf/2406.06369)
**Summary**: The paper investigates the alignment between Large Language Models (LLMs) like GPT-4 and human perceptions of conversational safety using the DICES dataset. GPT-4 shows a higher correlation with average human safety ratings compared to individual annotators, but the study highlights the need for larger datasets to assess potential disparities across demographic groups. The analysis also reveals significant variation within groups, indicating that race and gender alone do not fully explain differences in alignment.

### 240. [Test-Time Fairness and Robustness in Large Language Models](https://arxiv.org/pdf/2406.07685)
**Summary**: The paper introduces a novel approach to address biases in Large Language Models (LLMs) at test time, focusing on the limitations of existing implicit debiasing methods. It proposes a stratified invariance framework that allows for explicit debiasing requirements, ranging from population to individual levels, and introduces a data augmentation strategy and prompting technique to enforce this invariance. The approach effectively reduces bias in LLMs across various benchmarks without additional data or model adjustments.

### 241. [Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity](https://arxiv.org/pdf/2406.09790)
**Summary**: The paper investigates the limitations of current contrastive learning methods in achieving higher semantic textual similarity (STS) scores, identifying an upper limit of 87.5 for Spearman's correlation. To overcome this, the authors introduce Pcc-tuning, which uses Pearson's correlation coefficient as a loss function, significantly improving STS performance with minimal fine-grained annotations.

### 242. [Mixture-of-Skills: Learning to Optimize Data Usage for Fine-Tuning Large Language Models](https://arxiv.org/pdf/2406.08811)
**Summary**: The paper introduces Mixture-of-Skills (MoS), a reinforcement learning framework designed to optimize data usage during the fine-tuning of large language models (LLMs), addressing the challenges posed by heterogeneous and imbalanced datasets. MoS dynamically adjusts the focus on different datasets to ensure comprehensive skill development, and its effectiveness is demonstrated through extensive experiments on diverse LLM backbones and benchmarks. Additionally, the paper proposes MoSpec, an adaptation for task-specific fine-tuning, highlighting the importance of dataset rebalancing in LLM optimization.

### 243. [Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing](https://arxiv.org/pdf/2406.08464)
**Summary**: The paper introduces Magpie, a method for synthesizing high-quality instruction data by prompting aligned large language models (LLMs) like Llama-3-Instruct with partial templates, generating 4 million instructions and responses. The synthesized data, after filtering, is shown to perform comparably to the official Llama-3-8B-Instruct model on various benchmarks, demonstrating the potential for scalable and effective data synthesis without relying on private alignment data.

### 244. [Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs](https://arxiv.org/pdf/2406.11695)
**Summary**: The paper introduces MIPRO, an algorithm for optimizing prompts in multi-stage Language Model Programs (LMPs) by refining instructions and demonstrations without module-level labels or gradients. MIPRO employs program- and data-aware techniques, a stochastic evaluation function, and a meta-optimization procedure to enhance performance, achieving up to 13% higher accuracy compared to baseline optimizers on diverse LMP tasks using the Llama-3-8B model.

### 245. [Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning](https://arxiv.org/pdf/2406.12050)
**Summary**: The paper introduces "reflective augmentation," a novel technique that enhances language models' mathematical reasoning by embedding problem reflection into training instances. This method encourages the model to consider alternative perspectives and engage with abstractions, leading to improved performance in both standard and complex reasoning tasks. The approach is shown to be effective and complementary to existing data augmentation techniques.

### 246. [Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts](https://arxiv.org/pdf/2406.12034)
**Summary**: The paper introduces Self-MoE, a method that converts monolithic large language models (LLMs) into modular systems of self-specialized experts (MiXSE) using self-generated synthetic data. This approach enhances the LLM's performance across various tasks without requiring extensive human-labeled data, showing significant improvements (6.5% on average) over the base LLM in benchmarks. The study also emphasizes the benefits of modularity and self-improvement in creating efficient and adaptable systems.

### 247. [COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities](https://arxiv.org/pdf/2406.12074)
**Summary**: The paper introduces Community-Cross-Instruct, an unsupervised framework that aligns large language models (LLMs) to online communities by automatically generating instruction-output pairs from community discussions. This method allows for the fine-tuning of LLMs to accurately represent and evaluate the beliefs of specific communities, demonstrated through applications on Reddit, and offers a scalable alternative to traditional survey methods.

### 248. [What Matters in Memorizing and Recalling Facts? Multifaceted Benchmarks for Knowledge Probing in Language Models](https://arxiv.org/pdf/2406.12277)
**Summary**: The paper introduces BELIEF(ICL), a knowledge probing benchmark designed to evaluate the factual knowledge recall abilities of both encoder- and decoder-based pre-trained language models (PLMs) from multiple perspectives. The benchmark uses a multi-prompt dataset, MyriadLAMA, to assess accuracy, consistency, and reliability in recalling facts, and it investigates factors influencing knowledge recall in PLMs, such as model size and pretraining strategies.

### 249. [First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning](https://arxiv.org/pdf/2406.16078)
**Summary**: The paper investigates how language models (LMs) use heuristics during multi-step reasoning tasks, finding that LMs rely more on heuristics early in the process and shift to more rational strategies as they approach the final answer. This dynamic use of heuristics and rational reasoning suggests that LMs can effectively combine both approaches to improve performance in complex reasoning tasks.

### 250. [Insights into LLM Long-Context Failures: When Transformers Know but Don't Tell](https://arxiv.org/pdf/2406.14673)
**Summary**: The paper investigates the limitations of Large Language Models (LLMs) in handling long contexts, revealing that while these models encode positional information, they often fail to effectively utilize it for accurate responses. This "know but don't tell" phenomenon highlights a disconnect between information retrieval and utilization, with the study also examining the impact of extraction time on final accuracy, providing deeper insights into the mechanics of transformer models.

### 251. [When Parts Are Greater Than Sums: Individual LLM Components Can Outperform Full Models](https://arxiv.org/pdf/2406.13131)
**Summary**: The paper investigates in-context learning by analyzing the individual contributions of attention heads and MLPs within large language models. It identifies various types of components—good, bad, and label-biased—and demonstrates that reweighting these components based on a few labeled examples can significantly improve model performance. The study offers insights into the internal workings of language models and presents a practical method for enhancing in-context learning accuracy.

### 252. [FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models](https://arxiv.org/pdf/2406.16069)
**Summary**: The paper introduces FastMem, a method that enhances the context awareness of large language models (LLMs) by quickly memorizing the prompt before inference. By optimizing only the last Feed-Forward Network module, FastMem improves accuracy and reduces output structure failures, as demonstrated by significant gains in tasks like reading comprehension and text summarization.

### 253. [From Insights to Actions: The Impact of Interpretability and Analysis Research on NLP](https://arxiv.org/pdf/2406.12618)
**Summary**: The paper investigates the impact of interpretability and analysis (IA) research on the broader field of NLP, finding that IA work is well-cited and central in the NLP citation graph. Survey responses and manual annotations indicate that NLP researchers widely acknowledge the importance of IA findings for advancing the field, with many novel methods influenced by IA research. The study concludes by identifying gaps in current IA work and calls for more impactful future research.

### 254. [Native Design Bias: Studying the Impact of English Nativeness on Language Model Performance](https://arxiv.org/pdf/2406.17385)
**Summary**: The study examines whether large language models (LLMs) exhibit performance biases based on the nativeness of English speakers, finding that non-native English speakers receive lower-quality or factually incorrect responses more frequently. The research highlights a strong anchoring effect where the model's awareness of the user's nativeness further degrades response quality for non-native speakers, based on a dataset of over 12,000 annotations from diverse annotators.

### 255. [Mental Disorder Classification via Temporal Representation of Text](https://arxiv.org/pdf/2406.15470)
**Summary**: The paper introduces a novel framework for mental disorder classification by compressing chronological social media posts into a time-variant numerical representation, addressing the limitations of current language models in handling sequential text data. The framework outperforms state-of-the-art methods in predicting depression, self-harm, and anorexia, with a 5% improvement in F1 score, and also demonstrates the importance of temporal properties in text analysis. Additionally, the framework is applied in a cross-domain study to explore shared features across different mental disorders and the potential for inter-domain data usage.

### 256. [SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How to Fix It)](https://arxiv.org/pdf/2406.17975)
**Summary**: The paper reviews recent Membership Inference Attacks (MIAs) on Large Language Models (LLMs), highlighting concerns about distribution shifts due to post-hoc dataset construction. It proposes new evaluation methods, including randomized test splits and unique sequence injections, to address these issues and improve the validity of MIA benchmarks.

### 257. [Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models](https://arxiv.org/pdf/2406.17169)
**Summary**: The paper introduces Multi-LogiEval, a comprehensive evaluation dataset designed to assess the multi-step logical reasoning capabilities of Large Language Models (LLMs) across various inference rules and depths, including non-monotonic reasoning. The study finds that LLMs' performance significantly declines with increased reasoning depth, highlighting the need for further research to enhance their logical reasoning abilities.

### 258. [Make Some Noise: Unlocking Language Model Parallel Inference Capability through Noisy Training](https://arxiv.org/pdf/2406.17404)
**Summary**: The paper introduces the Make Some Noise (MSN) training framework, which enhances language model parallel decoding by introducing noise during training, eliminating the need for additional model structures or supervised fine-tuning. The authors also propose a tree-based retrieval-augmented Jacobi (TR-Jacobi) decoding strategy to further speed up inference. Experiments demonstrate that MSN improves inference speed by 2.3-2.7x without degrading model performance, achieving comparable acceleration to state-of-the-art models with additional structures.

### 259. [Detection and Measurement of Syntactic Templates in Generated Text](https://arxiv.org/pdf/2407.00211)
**Summary**: The paper introduces syntactic templates as a method to analyze the repetition and diversity of text generated by large language models (LLMs), finding that models frequently produce templated text that closely resembles pre-training data. The study reveals that syntactic templates can differentiate between models, tasks, and domains, providing a valuable tool for evaluating model behavior and memorization of training data.

### 260. [Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context NLP](https://arxiv.org/pdf/2407.00402)
**Summary**: The paper argues that grouping various long-context NLP tasks solely by input length is unproductive and proposes a more nuanced taxonomy based on two axes of difficulty: Diffusion (finding information) and Scope (amount of information). It highlights the under-exploration of tasks with highly diffused and extensive necessary information, calling for more informed research and benchmark design in long-context NLP.

### 261. [A Survey on Natural Language Counterfactual Generation](https://arxiv.org/pdf/2407.03993)
**Summary**: The paper surveys natural language counterfactual generation, a technique that modifies texts to change their classification outcomes, offering insights into model predictions and enhancing robustness. It categorizes methods into four groups and reviews evaluation metrics, highlighting ongoing challenges and future research directions.

### 262. [DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging](https://arxiv.org/pdf/2407.01470)
**Summary**: The paper introduces DogeRM, a framework that merges domain-specific knowledge into general reward models to improve performance in reinforcement learning from human feedback (RLHF). By integrating expert annotations through model merging, DogeRM reduces the need for costly preference data collection and enhances model alignment across various benchmarks.

### 263. [To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models](https://arxiv.org/pdf/2407.01920)
**Summary**: The paper introduces KnowUnDo, a benchmark for evaluating knowledge unlearning in Large Language Models (LLMs), focusing on the risk of excessive unlearning of sensitive data. It proposes MemFlex, a method that uses gradient information to precisely target and unlearn sensitive parameters, demonstrating superior performance in retaining general knowledge while effectively unlearning sensitive information.

### 264. [MalAlgoQA: Pedagogical Evaluation of Counterfactual Reasoning in Large Language Models and Implications for AI in Education](https://arxiv.org/pdf/2407.00938)
**Summary**: The paper introduces MalAlgoQA, a dataset designed to evaluate counterfactual reasoning in Large Language Models (LLMs) through mathematics and reading comprehension questions. It focuses on "malgorithms," flawed yet logically coherent rationales for incorrect answers, and introduces metrics like Malgorithm Identification Accuracy (MIA) to assess LLMs' ability to identify these flawed reasoning paths. The study finds that state-of-the-art LLMs struggle with MIA, indicating challenges in counterfactual reasoning, which is crucial for AI-powered educational tools.

### 265. [OffsetBias: Leveraging Debiased Data for Tuning Evaluators](https://arxiv.org/pdf/2407.06551)
**Summary**: The paper identifies six types of biases in judge models used to evaluate generated responses from Large Language Models (LLMs) and introduces EvalBiasBench, a collection of test cases to assess these biases. The authors propose methods to construct a debiased dataset, OffsetBias, and demonstrate that fine-tuning on this dataset improves the robustness and performance of judge models in various evaluation scenarios.

### 266. [MMedAgent: Learning to Use Medical Tools with Multi-modal Agent](https://arxiv.org/pdf/2407.02483)
**Summary**: The paper introduces MMedAgent, the first multi-modal agent specifically designed for the medical field, which leverages a curated dataset of medical tools to select the most appropriate tool for various tasks. MMedAgent outperforms existing open-source methods and even GPT-4 in medical tasks, demonstrating its efficiency in tool selection and integration.

### 267. [Cactus: Towards Psychological Counseling Conversations using Cognitive Behavioral Theory](https://arxiv.org/pdf/2407.03103)
**Summary**: The paper introduces Cactus, a multi-turn dialogue dataset designed to simulate real-life psychological counseling sessions using Cognitive Behavioral Therapy (CBT). By creating diverse client personas and systematically applying CBT techniques, the dataset aims to improve the accessibility of counseling through large language models. Experimental results show that a model trained on Cactus, named Camel, outperforms other models in counseling skills, indicating its potential as a counseling agent.

### 268. [Self-training Language Models for Arithmetic Reasoning](https://arxiv.org/pdf/2407.08400)
**Summary**: The paper investigates the effectiveness of self-training language models for arithmetic reasoning without additional annotated data, using automated feedback. It finds significant improvements in both offline and online self-training scenarios, with preference optimization methods outperforming traditional supervised training in online settings due to better stability and robustness.

### 269. [NativQA: Multilingual Culturally-Aligned Natural Query for LLMs](https://arxiv.org/pdf/2407.09823)
**Summary**: The paper introduces NativQA, a scalable, language-independent framework for creating culturally and regionally aligned QA datasets in native languages, addressing the lack of such resources for evaluating and fine-tuning large language models (LLMs). The authors demonstrate the framework's efficacy with MultiNativQA, a multilingual dataset of 64k QA pairs in seven languages from native speakers across nine regions, and benchmark LLMs using this dataset.

### 270. [Historical Ink: 19th Century Latin American Spanish Newspaper Corpus with LLM OCR Correction](https://arxiv.org/pdf/2407.12838)
**Summary**: The paper introduces a novel dataset of 19th-century Latin American newspaper texts, filling a critical gap in historical and linguistic research. Additionally, it presents a flexible framework using a Large Language Model for OCR error correction and linguistic surface form detection, which is applied to the new dataset, making it adaptable for various contexts and datasets.

### 271. [Knowledge-based Consistency Testing of Large Language Models](https://arxiv.org/pdf/2407.12830)
**Summary**: The paper introduces KonTest, an automated framework for evaluating the consistency and knowledge gaps in Large Language Models (LLMs) using a knowledge graph. KonTest identifies inconsistencies and knowledge gaps in LLMs, with a 16.5% gap detected across tested models, and demonstrates a 32.48% reduction in knowledge gaps through mitigation methods. The study also highlights GPT3.5's limited effectiveness in knowledge construction for consistency testing.

### 272. [Unlocking the Potential of Model Merging for Low-Resource Languages](https://arxiv.org/pdf/2407.03994)
**Summary**: The paper introduces model merging as an alternative to the conventional continual pre-training and supervised fine-tuning approach for adapting large language models (LLMs) to low-resource languages. By combining models with distinct capabilities, the authors demonstrate that model merging can effectively equip LLMs with task-solving abilities without requiring additional training data in the target languages. Their experiments with Llama-2-7B show that this method outperforms the CT-then-SFT approach, especially in scenarios with extremely limited data, and they propose enhancements to the merging process to further improve performance.

### 273. [Are Large Language Models Capable of Generating Human-Level Narratives?](https://arxiv.org/pdf/2407.13248)
**Summary**: The paper examines the storytelling abilities of Large Language Models (LLMs) by analyzing narrative development and plot progression through story arcs, turning points, and affective dimensions. It finds that human-written stories are more suspenseful and diverse compared to LLM-generated stories, which tend to be homogeneously positive and lack tension. The study concludes that LLMs generally fall short in narrative reasoning and suggests that integrating discourse features can significantly improve LLM storytelling.

### 274. [Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together](https://arxiv.org/pdf/2407.10930)
**Summary**: The paper introduces a novel approach called BetterTogether that combines fine-tuning and prompt optimization to enhance the performance of modular NLP pipelines, particularly in scenarios where intermediate labels or gradient flow are absent. By alternating between optimizing language model weights and prompt templates, the method achieves significant improvements in downstream task metrics, outperforming individual optimizations by up to 60% and 6% on average across various models and tasks.

### 275. [Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning with Knowledge-guided Retrieval Augmented Generation](https://arxiv.org/pdf/2407.10805)
**Summary**: The paper introduces Think-on-Graph 2.0 (ToG-2), a hybrid retrieval-augmented generation framework that enhances large language models by integrating knowledge graphs and document retrieval for deep and accurate reasoning. ToG-2 iteratively retrieves information from both structured and unstructured sources, improving context and graph retrieval, and demonstrates superior performance on knowledge-intensive datasets, even elevating smaller models to GPT-3.5's level.

### 276. [When Can Transformers Count to n?](https://arxiv.org/pdf/2407.15160)
**Summary**: The paper investigates whether transformer-based language models can perform basic counting tasks, specifically counting the occurrences of a token in a string. It finds that transformers can count up to a certain limit if their state dimension is linear in the context length, but beyond this limit, the task becomes theoretically and empirically intractable for size-limited transformers. The study highlights the importance of understanding the limitations of transformer models in solving simple tasks.

### 277. [Knowledge Mechanisms in Large Language Models: A Survey and Perspective](https://arxiv.org/pdf/2407.15017)
**Summary**: The paper surveys knowledge mechanisms in Large Language Models (LLMs), categorizing them into knowledge utilization and evolution. It explores how LLMs memorize, comprehend, apply, and create knowledge, as well as how knowledge evolves within individual and group models. The study also addresses the fragility of parametric knowledge and hypothesizes about potential "dark knowledge" challenges, aiming to guide future research on understanding and improving LLMs.

### 278. [A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives](https://arxiv.org/pdf/2407.15489)
**Summary**: The paper compares multilingual pretraining objectives, focusing on language modeling and translation, in a controlled environment to establish best practices for NLP. It finds that the choice of pretraining objective is influenced by the model architecture and that multilingual translation is highly effective under suitable conditions. The study emphasizes the importance of comparable training data and model architectures for meaningful comparisons.

### 279. [Counter Turing Test ($CT^2$): Investigating AI-Generated Text Detection for Hindi -- Ranking LLMs based on Hindi AI Detectability Index ($ADI_{hi}$)](https://arxiv.org/pdf/2407.15694)
**Summary**: The paper investigates the detection of AI-generated text in Hindi, evaluating the proficiency of 26 Large Language Models (LLMs) in generating Hindi text and assessing the effectiveness of five AGTD techniques on a new AI-generated Hindi news dataset. The study introduces the Hindi AI Detectability Index ($ADI_{hi}$) to measure the detectability of AI-generated text, providing insights into the evolving landscape of AI-generated eloquence in Hindi.

### 280. [ReAttention: Training-Free Infinite Context with Finite Attention Scope](https://arxiv.org/pdf/2407.15176)
**Summary**: The paper introduces **ReAttention**, a training-free method that allows Large Language Models (LLMs) to handle infinite context lengths with a finite attention scope, overcoming the limitations imposed by the self-attention mechanism. By performing position-agnostic top-$k$ attention before the standard position-aware self-attention, ReAttention enables LLMs to support context lengths of up to 1 million tokens and beyond, as demonstrated on models like LLaMA and Mistral, without requiring additional training. The approach is validated on benchmarks and shows competitive performance with traditional methods while significantly enhancing context length capabilities.

### 281. [Revisiting Who's Harry Potter: Towards Targeted Unlearning from a Causal Intervention Perspective](https://arxiv.org/pdf/2407.16997)
**Summary**: The paper introduces a new task of targeted unlearning for language models, aiming to remove specific information about a given target without affecting other data. It proposes a causal intervention framework to model the unlearning process, treating the target's knowledge as a confounder. The approach demonstrates competitive performance in experiments without explicit optimization for specific criteria, and the code is made available for further research.

### 282. [A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios](https://arxiv.org/pdf/2408.01963)
**Summary**: The paper introduces a novel metric to assess the robustness of large language models in non-adversarial scenarios, focusing on their insensitivity to meaning-preserving variations in input. By evaluating models on benchmark datasets with naturally occurring perturbations and semantically equivalent paraphrases, the authors demonstrate the effectiveness of their proposed metric in measuring model robustness.

### 283. [S2-Attention: Hardware-Aware Context Sharding Among Attention Heads](https://arxiv.org/pdf/2407.17678)
**Summary**: The paper introduces S2-Attention, a Triton library for hardware-aware sparse attention optimization, enabling heterogeneous sharding of context across attention heads. This approach achieves significant wall-clock speedups (up to 25.3X) over dense attention baselines while maintaining strong downstream performance and retrieval accuracy, especially beneficial for large language models.

### 284. [DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models](https://arxiv.org/pdf/2407.17023)
**Summary**: The paper introduces DynamicQA, a novel dataset designed to study intra-memory conflicts in Language Models (LMs), where conflicting knowledge within the model's parameters affects its ability to integrate relevant context. The dataset includes facts with temporal and disputable dynamics, allowing for the analysis of how different types of knowledge conflicts impact the model's performance. The study finds that LMs exhibit more intra-memory conflict with dynamic facts and that these conflicts are harder to resolve with additional context, suggesting challenges for retrieval-augmented generation methods.

### 285. [Adaptive Contrastive Decoding in Retrieval-Augmented Generation for Handling Noisy Contexts](https://arxiv.org/pdf/2408.01084)
**Summary**: The paper introduces Adaptive Contrastive Decoding (ACD) to enhance the performance of large language models in open-domain question answering by effectively managing noisy contexts. ACD improves robustness by ensuring that the model remains focused on relevant information, outperforming traditional methods in retrieval-augmented generation tasks.

### 286. [Optimal and efficient text counterfactuals using Graph Neural Networks](https://arxiv.org/pdf/2408.01969)
**Summary**: The paper introduces a framework for generating counterfactual explanations in NLP models using Graph Neural Networks, which creates semantically edited inputs that alter model predictions. The framework is tested on binary sentiment and topic classification tasks, demonstrating that it produces contrastive, fluent, and minimal edits while being significantly faster than existing methods.

### 287. [CMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models](https://arxiv.org/pdf/2407.17467)
**Summary**: The paper introduces the Critical Mixture Ratio (CMR) scaling law, which predicts the optimal balance between general and domain-specific data for continual pre-training of large language models (LLMs) to prevent catastrophic forgetting and enhance domain-specific performance. The study reveals a power-law relationship between loss, mixture ratio, and training tokens, providing a practical guideline for efficiently managing training resources and ensuring both general and domain-specific capabilities in LLMs.

### 288. [Mathfish: Evaluating Language Model Math Reasoning via Grounding in Educational Curricula](https://arxiv.org/pdf/2408.04226)
**Summary**: The paper introduces MathFish, a framework for evaluating language models' mathematical reasoning by grounding it in K-12 educational curricula. It contributes two datasets: one detailing math skills and concepts, and another with math problems labeled by these standards. The study finds that while models can predict standards close to the ground truth, they often fail to fully align with educational standards, highlighting the need for careful evaluation in curricular material generation.

### 289. [Quantifying the Role of Textual Predictability in Automatic Speech Recognition](https://arxiv.org/pdf/2407.16537)
**Summary**: The paper introduces a novel method to quantify the impact of textual predictability on automatic speech recognition (ASR) errors, represented by a single metric, $k$. This approach is used to compare the performance of Wav2Vec 2.0 and hybrid ASR models, revealing that Wav2Vec 2.0 effectively utilizes textual context despite lacking an explicit language model. The study also highlights that poor performance on African-American English is primarily due to acoustic-phonetic modeling issues, suggesting the method's utility in diagnosing and enhancing ASR systems.

### 290. [Investigating Critical Period Effects in Language Acquisition through Neural Language Models](https://arxiv.org/pdf/2407.19325)
**Summary**: The study investigates whether critical period effects in language acquisition are innate or result from experience by using neural language models. The models, which lack innate maturational stages, do not exhibit critical period effects when exposed to a second language later in "learning." This suggests that critical period effects may be due to innate mechanisms rather than solely statistical learning. The study also demonstrates that simulating a decrease in plasticity through regularization can mimic these effects, implying that additional factors beyond L1 learning are necessary for cognitive plausibility in language models.

### 291. [Counterfactuals As a Means for Evaluating Faithfulness of Attribution Methods in Autoregressive Language Models](https://arxiv.org/pdf/2408.11252)
**Summary**: The paper introduces a novel method for evaluating the faithfulness of attribution methods in autoregressive language models by using counterfactual generation. This approach creates fluent, in-distribution counterfactuals, addressing the issue of out-of-distribution inputs that arise from traditional faithfulness evaluation techniques, thereby enhancing the reliability of the evaluation process.

### 292. [HySem: A context length optimized LLM pipeline for unstructured tabular extraction](https://arxiv.org/pdf/2408.09434)
**Summary**: The paper introduces HySem, a pipeline designed to extract and semantically represent tabular data from HTML tables, addressing the challenges of diverse table presentations and context length limitations in Large Language Models (LLMs). HySem employs a context length optimization technique to generate accurate semantic JSON representations, outperforming open-source models and providing competitive performance against OpenAI GPT-4, making it suitable for cost- and privacy-sensitive pharmaceutical enterprises.

### 293. [Rater Cohesion and Quality from a Vicarious Perspective](https://arxiv.org/pdf/2408.08411)
**Summary**: The paper investigates the use of vicarious annotation to reduce rater disagreement in human-centered AI systems, particularly in politically charged contexts. It examines how political affiliations and demographic backgrounds affect rater perceptions of offense and evaluates rater quality using CrowdTruth metrics, focusing on in-group and cross-group cohesion at both personal and vicarious annotation levels.

### 294. [MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and Iterative Sub-SQL Refinement for Text-to-SQL](https://arxiv.org/pdf/2408.07930)
**Summary**: The paper introduces MAG-SQL, a multi-agent generative approach for the Text-to-SQL task, which addresses the challenges of complex database schemas and difficult questions by employing soft schema linking and iterative Sub-SQL refinement. The framework includes novel methods for column selection and question decomposition, and it introduces external oversight for each generation step. MAG-SQL demonstrates significant improvements over existing models, achieving an execution accuracy of 61.08% on the BIRD benchmark with GPT-4, surpassing the baseline performance of both vanilla GPT-4 and MAC-SQL.

### 295. [Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models](https://arxiv.org/pdf/2408.14866)
**Summary**: The paper introduces DeGCG, a two-stage transfer learning framework for improving the efficiency of adversarial suffix generation in large language models (LLMs). By decoupling the search process into pre-searching and post-searching stages, DeGCG enhances suffix transferability across models and datasets. The interleaved variant, i-DeGCG, further accelerates the search process by leveraging self-transferability, achieving superior performance on Llama2-chat-7b compared to baseline methods.

### 296. [Correcting FLORES Evaluation Dataset for Four African Languages](https://arxiv.org/pdf/2409.00626)
**Summary**: The paper addresses inconsistencies and inaccuracies in the FLORES evaluation dataset for Hausa, Northern Sotho (Sepedi), Xitsonga, and isiZulu, which could affect NLP tasks like machine translation. Through a review by native speakers, numerous corrections were made, enhancing the dataset's quality and reliability. The authors advocate for the inclusion of native speakers in future translation projects to ensure linguistic accuracy and cultural relevance.

### 297. [Propaganda to Hate: A Multimodal Analysis of Arabic Memes with Multi-Agent LLMs](https://arxiv.org/pdf/2409.07246)
**Summary**: The paper investigates the intersection of propaganda and hate in Arabic memes using a multi-agent large language model (LLM) approach. By extending a propagandistic meme dataset with hate labels, the study finds an association between propaganda and hate in memes, providing a baseline for future research and making the experimental resources available to the community.

### 298. [Native vs Non-Native Language Prompting: A Comparative Analysis](https://arxiv.org/pdf/2409.07054)
**Summary**: The study compares native and non-native language prompting strategies across 11 NLP tasks using Arabic datasets, involving 197 experiments with three large language models. The findings indicate that non-native prompts generally yield better performance, followed by mixed and native prompts, highlighting the importance of prompt language in eliciting effective responses from LLMs, especially for low-resourced languages.

### 299. [UPCS: Unbiased Persona Construction for Dialogue Generation](https://arxiv.org/pdf/2409.05257)
**Summary**: The paper introduces the UPCS framework, which aims to create unbiased persona profiles for dialogue generation by categorizing character descriptions into eight dimensions, including bias mitigation strategies. Experimental results show that UPCS outperforms existing methods in terms of accuracy, diversity, bias elimination, and user satisfaction, significantly enhancing the reliability of narrative systems.

### 300. [Evidence-backed Fact Checking using RAG and Few-Shot In-Context Learning with LLMs](https://arxiv.org/pdf/2408.12060)
**Summary**: The paper introduces an automated fact-checking system that leverages the Averitec dataset and a Retrieve and Generate (RAG) pipeline to extract evidence for claims, which is then used by large language models (LLMs) for classification. The system demonstrates significant improvement over the baseline, achieving an 'Averitec' score of 0.33, and evaluates the few-shot In-Context Learning capabilities of various LLMs.

### 301. [WinoPron: Revisiting English Winogender Schemas for Consistency, Coverage, and Grammatical Case](https://arxiv.org/pdf/2409.05653)
**Summary**: The paper addresses issues in the Winogender Schemas dataset, which is used to evaluate gender bias in coreference resolution, by identifying inconsistencies, template violations, and typographical errors. The authors introduce WinoPron, a revised dataset, and use it to evaluate coreference resolution systems, finding that accusative pronouns are particularly challenging. They also propose a new method to assess pronominal bias beyond binary gender distinctions, revealing variations in bias across different pronoun forms.

### 302. [FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists](https://arxiv.org/pdf/2409.12832)
**Summary**: The paper introduces FoodPuzzle, a benchmark for developing large language model agents to generate hypotheses in flavor science, addressing the need for rapid innovation in the food industry. By integrating in-context learning and retrieval augmented techniques, the proposed Scientific Agent significantly outperforms traditional methods in predicting flavor profiles, suggesting a transformative approach to flavor development.

### 303. [Deconfounded Causality-aware Parameter-Efficient Fine-Tuning for Problem-Solving Improvement of LLMs](https://arxiv.org/pdf/2409.02686)
**Summary**: The paper investigates the reasoning limitations of Large Language Models (LLMs) and proposes a novel parameter-efficient fine-tuning method called Deconfounded Causal Adaptation (DCA) to enhance their problem-solving capabilities. By formulating the reasoning process into a causal framework and visualizing the text generation, the authors demonstrate that DCA significantly improves LLM performance across benchmarks with minimal tunable parameters, achieving better or comparable results to other fine-tuning methods.

### 304. [IndicVoices-R: Unlocking a Massive Multilingual Multi-speaker Speech Corpus for Scaling Indian TTS](https://arxiv.org/pdf/2409.05356)
**Summary**: The paper introduces IndicVoices-R (IV-R), a massive multilingual Indian TTS dataset derived from ASR data, featuring 1,704 hours of high-quality speech from 10,496 speakers across 22 languages. The authors demonstrate improved zero-shot speaker generalization by fine-tuning an English pre-trained model on a combined dataset of IndicTTS and IV-R, addressing the scarcity of high-quality TTS data for Indian languages.

### 305. [Enhancing adversarial robustness in Natural Language Inference using explanations](https://arxiv.org/pdf/2409.07423)
**Summary**: The paper investigates enhancing adversarial robustness in Natural Language Inference (NLI) by using natural language explanations as a defense strategy. By fine-tuning a classifier on explanations rather than premise-hypothesis pairs, the model achieves greater robustness against adversarial attacks compared to traditional methods. The study also explores the correlation between language generation metrics and human perception to assess the validity of generated explanations, ensuring a more robust NLI model.

### 306. [The Faetar Benchmark: Speech Recognition in a Very Under-Resourced Language](https://arxiv.org/pdf/2409.08103)
**Summary**: The paper introduces the Faetar Automatic Speech Recognition Benchmark, a challenging dataset for low-resource speech recognition, focusing on the under-resourced Franco-Provençal variety spoken in Italy. The corpus, consisting of 5 hours of transcribed and 20 hours of unlabelled speech, is used to evaluate state-of-the-art multilingual models, achieving a best phone error rate of 30.4% after continued pre-training on the unlabelled data.

### 307. [Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method](https://arxiv.org/pdf/2409.14781)
**Summary**: The paper introduces a divergence-based calibration method for pretraining data detection in large language models (LLMs), addressing limitations of the Min-K% Prob method by using cross-entropy to measure divergence between token probability and frequency distributions. This approach significantly improves detection accuracy, as demonstrated by experiments on both English and Chinese benchmarks, including the newly developed PatentMIA dataset.

### 308. [Beyond Persuasion: Towards Conversational Recommender System with Credible Explanations](https://arxiv.org/pdf/2409.14399)
**Summary**: The paper introduces PC-CRS, a method that enhances the credibility of explanations in conversational recommender systems (CRS) by guiding explanation generation with credibility-aware persuasive strategies and refining them through post-hoc self-reflection. Experimental results show that PC-CRS effectively promotes both persuasive and credible explanations, and further analysis suggests that credible explanations can improve recommendation accuracy.

### 309. [Obliviate: Neutralizing Task-agnostic Backdoors within the Parameter-efficient Fine-tuning Paradigm](https://arxiv.org/pdf/2409.14119)
**Summary**: The paper introduces Obliviate, a defense mechanism for neutralizing task-agnostic backdoors in parameter-efficient fine-tuning (PEFT) of large language models. The proposed method employs techniques to amplify benign neurons and penalize trigger tokens, effectively reducing the success rate of state-of-the-art backdoor attacks by 83.6%. Obliviate also demonstrates robust defense against task-specific backdoors and adaptive attacks.

### 310. [PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead](https://arxiv.org/pdf/2409.19745)
**Summary**: The paper introduces PEAR, a method that enhances the context awareness of large language models (LLMs) in retrieval-augmented generation (RAG) tasks without adding any inference overhead. PEAR identifies and re-weights attention heads that suppress context awareness, optimizing their impact through learnable coefficients, and it is position-embedding-agnostic, making it widely applicable and efficient.

### 311. [From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging](https://arxiv.org/pdf/2410.01215)
**Summary**: The paper introduces Multi-Granularity Debugger (MGDebugger), a hierarchical code debugging system that addresses errors at multiple levels of granularity, from syntax to algorithmic flaws. By decomposing code into a tree structure of subfunctions and using an LLM-simulated Python executor to trace execution, MGDebugger iteratively resolves bugs in a bottom-up manner. Experiments show it significantly outperforms existing systems, achieving higher accuracy and repair success rates across various bug categories and difficulty levels.

### 312. [KV-Compress: Paged KV-Cache Compression with Variable Compression Rates per Attention Head](https://arxiv.org/pdf/2410.00161)
**Summary**: The paper introduces KV-Compress, a novel method for compressing key-value (KV) cache in large language models (LLMs) to efficiently support long-context inference. By evicting contiguous KV blocks within a PagedAttention framework, KV-Compress achieves state-of-the-art compression rates, reducing memory usage by up to 8x with minimal performance impact and enabling higher throughput in decoding batches.

### 313. [IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation](https://arxiv.org/pdf/2409.18892)
**Summary**: The paper introduces IDGen, a framework for generating discriminative prompts to evaluate Large Language Models (LLMs) based on Item Discrimination (ID) theory from educational assessment. IDGen aims to create challenging and specific prompts that reveal performance differences between models, with a self-correct mechanism and predictive models for prompt discrimination and difficulty. The generated data is shown to be more challenging and discriminative than previous methods, with plans to release a dataset of over 3,000 prompts for LLM evaluation research.

### 314. [Visual Question Decomposition on Multimodal Large Language Models](https://arxiv.org/pdf/2409.19339)
**Summary**: The paper investigates the question decomposition capabilities of Multimodal Large Language Models (MLLMs) and introduces a systematic evaluation framework to assess the quality of decomposed sub-questions. It identifies limitations in current MLLMs and proposes a finetuning dataset, DecoVQA+, and an efficient finetuning pipeline to enhance the models' ability to selectively decompose questions, leading to improved accuracy on Visual Question Answering (VQA) benchmarks.

### 315. [What the Harm? Quantifying the Tangible Impact of Gender Bias in Machine Translation with a Human-centered Study](https://arxiv.org/pdf/2410.00545)
**Summary**: The paper investigates the tangible impact of gender bias in machine translation (MT) through a human-centered study involving 90 participants who post-edited MT outputs to ensure correct gender translation. The study reveals that feminine post-editing requires significantly more effort, time, and financial costs compared to masculine translations, highlighting the need for human-centered approaches to assess and mitigate the societal impact of MT bias.

### 316. [MetaMetrics: Calibrating Metrics For Generation Tasks Using Human Preferences](https://arxiv.org/pdf/2410.02381)
**Summary**: The paper introduces MetaMetrics, a calibrated meta-metric designed to evaluate generation tasks by optimizing the combination of existing metrics to better align with human preferences across different modalities. It demonstrates effectiveness in both language and vision tasks, showing significant benefits in multilingual and multi-domain scenarios, making it a versatile tool for improving the evaluation of generation tasks.

### 317. [Aligning with Logic: Measuring, Evaluating and Improving Logical Consistency in Large Language Models](https://arxiv.org/pdf/2410.02205)
**Summary**: The paper introduces a framework to measure and improve the logical consistency of Large Language Models (LLMs), which is crucial for their reliability and trustworthiness. It proposes three fundamental proxies—transitivity, commutativity, and negation invariance—to quantify logical consistency and evaluates various LLMs. The study also presents a data refinement technique to enhance logical consistency while maintaining alignment with human preferences, demonstrating its impact on LLM-based logic-dependent algorithms.

### 318. [Better Instruction-Following Through Minimum Bayes Risk](https://arxiv.org/pdf/2410.02902)
**Summary**: The paper explores using Minimum Bayes Risk (MBR) decoding with reference-based LLM judges to improve the performance of instruction-following LLMs, finding significant gains over traditional decoding methods. Additionally, the authors investigate iterative self-training on MBR-decoded outputs, which leads to performance improvements that often match or exceed the base models' MBR decoding performance.

### 319. [OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data](https://arxiv.org/pdf/2410.01560)
**Summary**: The paper introduces OpenMathInstruct-2, a massive open-source dataset for mathematical reasoning, consisting of 14M question-solution pairs, significantly larger than previous datasets. Through ablation experiments, the authors identify key factors affecting dataset quality, such as solution format and question diversity, and demonstrate that finetuning with OpenMathInstruct-2 improves performance on the MATH benchmark by 15.9%. The dataset, code, and models are released under a permissive license to support further research.

### 320. [Measuring and Improving Persuasiveness of Large Language Models](https://arxiv.org/pdf/2410.02653)
**Summary**: The paper introduces PersuasionBench and PersuasionArena, the first large-scale benchmarks for measuring the persuasiveness of large language models (LLMs). It finds that while larger models tend to be more persuasive, targeted training can significantly enhance the persuasive capabilities of smaller models, challenging the assumption that scale alone determines effectiveness. The study emphasizes the need for more comprehensive metrics to assess AI's societal impact, beyond computational power.

### 321. [Contextual Document Embeddings](https://arxiv.org/pdf/2410.02525)
**Summary**: The paper introduces contextual document embeddings, arguing that traditional dense embeddings are insufficiently contextual for retrieval tasks. It proposes two methods: a contrastive learning objective that considers neighboring documents and a contextual architecture that encodes neighbor information. These methods outperform traditional biencoders, especially in out-of-domain scenarios, achieving state-of-the-art results on the MTEB benchmark without complex techniques.

### 322. [LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations](https://arxiv.org/pdf/2410.02707)
**Summary**: The paper investigates the internal representations of large language models (LLMs) and finds that these models encode more information about the truthfulness of their outputs than previously recognized. It highlights that error detection based on these internal states is dataset-specific and that LLMs can predict the types of errors they are likely to make. The study also reveals a discrepancy between internal encoding and external behavior, suggesting that LLMs may know the correct answer but still produce incorrect outputs, which could inform future research on error analysis and mitigation.

### 323. [Efficient Model-Agnostic Multi-Group Equivariant Networks](https://arxiv.org/pdf/2310.09675)
**Summary**: The paper introduces efficient model-agnostic equivariant network designs for scenarios with multiple input groups or large product groups acting on a single input. It proposes novel fusion layers, called IS layers, which satisfy invariance-symmetry constraints and are shown to be universal approximators. The designs are tested on various tasks, demonstrating robustness and computational efficiency compared to existing methods.

### 324. [Reconstruct Your Previous Conversations! Comprehensively Investigating Privacy Leakage Risks in Conversations with GPT Models](https://arxiv.org/pdf/2402.02987)
**Summary**: The paper introduces a Conversation Reconstruction Attack that aims to extract private conversation content between users and GPT models through malicious prompts. Despite GPT-4's resilience, advanced attacks reveal significant privacy leakage across all models. The study underscores the vulnerability of GPT models to privacy breaches and calls for stronger defenses to protect user data.

### 325. [EffiBench: Benchmarking the Efficiency of Automatically Generated Code](https://arxiv.org/pdf/2402.02037)
**Summary**: The paper introduces EffiBench, a benchmark designed to evaluate the efficiency of code generated by large language models (LLMs) in solving 1,000 efficiency-critical coding problems from LeetCode. The study finds that the efficiency of LLM-generated code is generally inferior to human-written solutions, with GPT-4 producing code that, on average, takes 3.12 times longer to execute and, in extreme cases, up to 13.89 times longer and 43.92 times more memory-intensive.

### 326. [Typing to Listen at the Cocktail Party: Text-Guided Target Speaker Extraction](https://arxiv.org/pdf/2310.07284)
**Summary**: The paper introduces LLM-TSE, a novel text-guided target speaker extraction method that leverages LLaMA 2 to process user-typed text for semantic cues, addressing privacy concerns and reducing reliance on voiceprints. Experimental results demonstrate competitive performance with text-based cues alone and achieve a new state-of-the-art when combined with pre-registered cues, marking the first integration of large language models with TSE and offering a versatile, privacy-conscious solution to the cocktail party problem.

### 327. [Decoding Intelligence: A Framework for Certifying Knowledge Comprehension in LLMs](https://arxiv.org/pdf/2402.15929)
**Summary**: The paper introduces a novel framework for certifying the knowledge comprehension capabilities of Large Language Models (LLMs) with formal probabilistic guarantees. It provides high-confidence bounds on the probability of correct answers to knowledge comprehension prompts, leveraging knowledge graphs like Wikidata5m, and demonstrates that model performance improves with increased size.

### 328. [Comparing large language models and human programmers for generating programming code](https://arxiv.org/pdf/2403.00894)
**Summary**: The study compares the performance of seven large language models, with GPT-4 significantly outperforming others in generating programming code across various tasks and languages. GPT-4, using optimized prompt strategies, surpasses 85% of human participants in coding contests and shows strong code translation and error correction abilities, suggesting its potential as a reliable assistant in software development.

### 329. [Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections](https://arxiv.org/pdf/2402.16973)
**Summary**: The paper introduces HEAR, a system that guides humans in simulated residential environments by highlighting potential errors in its instructions and suggesting corrections, thereby improving decision-making. Evaluation with 80 users shows a 13% increase in success rate and a 29% reduction in error distance compared to traditional instruction-only systems, demonstrating the practical benefits of uncertainty communication in complex tasks.

### 330. [READ: Recurrent Adapter with Partial Video-Language Alignment for Parameter-Efficient Transfer Learning in Low-Resource Video-Language Modeling](https://arxiv.org/pdf/2312.06950)
**Summary**: The paper introduces READ, a novel Recurrent Adapter with Partial Video-Language Alignment, designed to improve parameter-efficient transfer learning in low-resource video-language modeling tasks. By incorporating recurrent computation for temporal modeling and using partial optimal transport for alignment, READ effectively captures temporal relations and preserves task-related information, outperforming existing fine-tuning strategies on multiple benchmarks.

### 331. [Rethinking the Role of Proxy Rewards in Language Model Alignment](https://arxiv.org/pdf/2402.03469)
**Summary**: The paper investigates the role of proxy rewards in aligning Large Language Models (LLMs) with human values through a process called "reverse reward engineering." By creating interpretable features as a white-box reward function, the authors aim to replicate the ground truth reward signal, finding that successful emulation requires responses to be relevant, sufficiently long for open-ended questions, and consistent for closed-ended questions. The resulting models demonstrate competitive performance in alignment benchmarks, suggesting that the white-box reward could serve as a strong baseline for LLM alignment without the need for extensive human feedback or reward model training.

### 332. [Creative Beam Search: LLM-as-a-Judge For Improving Response Generation](https://arxiv.org/pdf/2405.00099)
**Summary**: The paper introduces Creative Beam Search, a method that combines Diverse Beam Search with a Large Language Model (LLM) acting as a judge to enhance response generation in creative tasks. The approach aims to mimic human-like intentionality and creativity in machine-generated responses, outperforming standard sampling techniques. The study highlights the importance of a validation step to complement the generation process, ensuring higher quality outputs.

### 333. [A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models](https://arxiv.org/pdf/2403.12025)
**Summary**: The paper introduces a toolbox and methodologies for identifying biases in large language models (LLMs) that could harm health equity, using Med-PaLM 2 as a case study. It proposes a multifactorial framework for human assessment and a dataset, EquityMedQA, to evaluate LLM-generated medical answers. The study highlights the importance of diverse assessment methods and rater backgrounds to effectively surface biases, emphasizing the need for comprehensive approaches to ensure equitable healthcare outcomes.

### 334. ["I Like Sunnie More Than I Expected!": Exploring User Expectation and Perception of an Anthropomorphic LLM-based Conversational Agent for Well-Being Support](https://arxiv.org/pdf/2405.13803)
**Summary**: The study investigates how users' expectations and perceptions of two LLM-based mental well-being support systems differ, with one system (Sunnie) featuring an anthropomorphic design. Results indicate that both systems exceeded users' expectations in utility, but Sunnie, with its anthropomorphic elements, significantly outperformed the non-anthropomorphic system in fostering relational warmth, suggesting the potential of such designs in enhancing mental health support.

### 335. [SpinQuant: LLM quantization with learned rotations](https://arxiv.org/pdf/2405.16406)
**Summary**: The paper introduces SpinQuant, a novel approach to quantizing Large Language Models (LLMs) that uses learned rotation matrices to enhance quantization accuracy, particularly in the presence of outliers. SpinQuant significantly reduces the accuracy gap in zero-shot reasoning tasks compared to other quantization methods, achieving near-full-precision performance with 4-bit quantization on models like LLaMA-2 7B and LLaMA-3 8B.

### 336. [Implicit Multimodal Alignment: On the Generalization of Frozen LLMs to Multimodal Inputs](https://arxiv.org/pdf/2405.16700)
**Summary**: The paper investigates how frozen Large Language Models (LLMs) generalize to multimodal inputs, finding that perceptual and textual tokens are implicitly aligned within the model architecture, which aids generalization. This implicit multimodal alignment (IMA) is linked to model performance and can be used as a proxy for evaluation, while also revealing that hallucinations are due to misalignment between internal representations. The study also proposes methods to reduce inference costs and compress models by leveraging the stability of perceptual tokens across layers.

### 337. [Many-Shot In-Context Learning in Multimodal Foundation Models](https://arxiv.org/pdf/2405.09798)
**Summary**: The paper investigates the performance of multimodal foundation models, such as GPT-4o and Gemini 1.5 Pro, in many-shot in-context learning (ICL) across various domains and tasks. It finds that many-shot ICL, with up to nearly 2,000 examples, significantly outperforms few-shot ICL, with Gemini 1.5 Pro showing log-linear improvement. The study also highlights differences between open and closed models and explores the benefits of batching queries to reduce costs and latency.

### 338. [Self-Play Preference Optimization for Language Model Alignment](https://arxiv.org/pdf/2405.00675)
**Summary**: The paper introduces Self-Play Preference Optimization (SPPO), a novel method for aligning language models by treating the problem as a constant-sum two-player game. SPPO iteratively updates policies to approximate the Nash equilibrium, using a preference model with only 0.4B parameters to achieve state-of-the-art performance on various benchmarks without additional external supervision. The approach demonstrates superior performance compared to existing methods like DPO and IPO.

### 339. [Efficient Prompting for LLM-based Generative Internet of Things](https://arxiv.org/pdf/2406.10382)
**Summary**: The paper introduces a Large Language Model-based Generative Internet of Things (GIoT) system designed for local network deployment, addressing the limitations of open-source LLMs through prompt engineering and the creation of a Prompt Management Module and a Post-processing Module. The system is demonstrated to be effective in handling complex Table Question Answering tasks, achieving competitive performance compared to state-of-the-art LLMs without the need for additional training.

### 340. [Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition](https://arxiv.org/pdf/2406.02925)
**Summary**: The paper introduces a novel approach using task vector arithmetic to mitigate the synthetic-to-real gap in automatic speech recognition (ASR) models trained on synthetic data. The proposed SYN2REAL task vector method demonstrates a significant 10.03% reduction in word error rate on the SLURP dataset compared to baseline methods. Additionally, averaging SYN2REAL task vectors across multiple real speech domains enhances the model's adaptability to target text domains.

### 341. [ColPali: Efficient Document Retrieval with Vision Language Models](https://arxiv.org/pdf/2407.01449)
**Summary**: The paper introduces ColPali, a novel retrieval model that leverages Vision Language Models to efficiently exploit visual cues in documents, outperforming existing systems in retrieval tasks. It also presents the Visual Document Retrieval Benchmark ViDoRe to evaluate the performance of retrieval systems on visually rich documents, highlighting the need for models like ColPali that can handle complex document structures.

### 342. ["You Gotta be a Doctor, Lin": An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations](https://arxiv.org/pdf/2406.12232)
**Summary**: The study investigates biases in employment recommendations by Large Language Models (LLMs) like GPT-3.5-Turbo and Llama 3-70B-Instruct, finding a preference for candidates with White female-sounding names across various occupations. Salary recommendations also show significant variation based on name-related demographics, highlighting discrepancies with real-world labor data and the need for further scrutiny of LLM-powered systems.

### 343. [On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning](https://arxiv.org/pdf/2406.11823)
**Summary**: The paper investigates the challenges of creating efficient language and vision models for visually-situated natural language understanding, focusing on balancing model size with computational demands. By optimizing dataset formulation, vision modules, and supervision techniques, the study achieves improved inference throughput and performance, with models ranging from 160M to 13B parameters. The research will be fully open-sourced to promote transparency and reproducibility.

### 344. [WellDunn: On the Robustness and Explainability of Language Models and Large Language Models in Identifying Wellness Dimensions](https://arxiv.org/pdf/2406.12058)
**Summary**: The paper "WellDunn: On the Robustness and Explainability of Language Models and Large Language Models in Identifying Wellness Dimensions" evaluates the performance and explainability of language models (LMs) and large language models (LLMs) in mental health applications, focusing on their attention mechanisms and alignment with expert-labeled explanations. The study reveals that despite their advanced capabilities, LMs and LLMs often fall short in robustness and explainability, particularly in mental health contexts, underscoring the need for improved consistency and domain-specific knowledge in these models.

### 345. [Unlocking Continual Learning Abilities in Language Models](https://arxiv.org/pdf/2406.17245)
**Summary**: The paper introduces MIGU, a rehearsal-free and task-label-free method for continual learning in language models, which updates model parameters based on the magnitude of outputs in linear layers. MIGU leverages inherent model behaviors to mitigate catastrophic forgetting and achieves state-of-the-art performance across various language model architectures and continual learning benchmarks.

### 346. [BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions](https://arxiv.org/pdf/2406.15877)
**Summary**: The paper introduces BigCodeBench, a benchmark designed to evaluate the ability of Large Language Models (LLMs) to generate code for challenging and practical tasks by utilizing diverse function calls from 139 libraries across 7 domains. The benchmark includes 1,140 fine-grained tasks with rigorous testing, and a variant, BigCodeBench-Instruct, focuses on complex instructions. The evaluation of 60 LLMs reveals that while they can handle simple tasks, they struggle with complex instructions and precise function calls, achieving only up to 60% accuracy compared to human performance of 97%.

### 347. [Automated Progressive Red Teaming](https://arxiv.org/pdf/2407.03876)
**Summary**: The paper introduces Automated Progressive Red Teaming (APRT), a framework designed to identify vulnerabilities in large language models (LLMs) by automating the generation of adversarial prompts. APRT uses three core modules to progressively explore and exploit LLM weaknesses, and introduces a new metric, Attack Effectiveness Rate (AER), to evaluate the likelihood of eliciting unsafe but helpful responses. Experiments show APRT effectively exposes vulnerabilities in both open-source and closed-source LLMs, with significant success rates across different models.

### 348. [Can Large Language Models Understand Symbolic Graphics Programs?](https://arxiv.org/pdf/2408.08313)
**Summary**: The paper investigates the ability of large language models (LLMs) to understand symbolic graphics programs, which require spatial-semantic reasoning without relying on vision encoders. By creating a benchmark for semantic visual understanding, the authors evaluate LLMs and introduce Symbolic Instruction Tuning (SIT) to enhance their performance, finding that SIT improves both understanding of symbolic programs and general reasoning abilities.

### 349. [Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language Models](https://arxiv.org/pdf/2407.12327)
**Summary**: The paper introduces Spectra, a comprehensive suite of Large Language Models (LLMs) including Ternary Language Models (TriLMs), Quantized Language Models (QuantLMs), and traditional Floating-Point Language Models (FloatLMs). It demonstrates that TriLMs, despite using fewer bits, outperform their QuantLM and FloatLM counterparts, especially at scales exceeding one billion parameters. The study highlights the potential for low-bitwidth models to be more efficient and scalable, and releases over 500 intermediate checkpoints to aid further research.

### 350. [Residual Stream Analysis with Multi-Layer SAEs](https://arxiv.org/pdf/2409.04185)
**Summary**: The paper introduces Multi-Layer Sparse Autoencoders (MLSAEs), which are trained on residual stream activations from all transformer layers, allowing for a unified analysis of information flow across layers. The study finds that latent activations often remain active at a single layer per token, with significant variance across tokens, and that larger models exhibit more multi-layer activations. The findings provide insights into how representations evolve in transformers and are supported by code released for further analysis.

### 351. [mDPO: Conditional Preference Optimization for Multimodal Large Language Models](https://arxiv.org/pdf/2406.11839)
**Summary**: The paper introduces mDPO, a novel approach to conditional preference optimization for multimodal large language models (LLMs), addressing the issue of unconditional preference in existing methods. By incorporating image preferences and introducing a reward anchor, mDPO significantly enhances model performance, particularly in reducing hallucination, as demonstrated across various benchmarks.

### 352. [Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models](https://arxiv.org/pdf/2408.08926)
**Summary**: The paper introduces Cybench, a framework for evaluating the cybersecurity capabilities of language models through 40 professional-level Capture the Flag (CTF) tasks. The framework includes subtasks to break down complex challenges, and it evaluates eight models, finding that some, like Claude 3.5 Sonnet and GPT-4o, can solve tasks comparable to human performance in cybersecurity.

### 353. [An Adversarial Perspective on Machine Unlearning for AI Safety](https://arxiv.org/pdf/2409.18025)
**Summary**: The paper examines the effectiveness of machine unlearning methods in removing hazardous capabilities from large language models, challenging the distinction between unlearning and traditional safety post-training. It demonstrates that existing jailbreak techniques can bypass unlearning protections and introduces adaptive methods to recover unlearned capabilities, questioning the robustness of current unlearning approaches.

### 354. [CBF-LLM: Safe Control for LLM Alignment](https://arxiv.org/pdf/2408.15625)
**Summary**: The paper introduces CBF-LLM, a control-based framework that uses control barrier functions (CBFs) to align large language models (LLMs) and ensure safe text generation. By applying a safety filter to the token sequence output of a baseline LLM, the framework reduces the need for interventions in user-specified alignment tasks, as demonstrated through experiments with Llama 3 and RoBERTa models.

### 355. [The Crucial Role of Samplers in Online Direct Preference Optimization](https://arxiv.org/pdf/2409.19605)
**Summary**: The paper investigates the impact of different sampling strategies on the convergence rates of Direct Preference Optimization (DPO), finding that uniform sampling leads to linear convergence, while an online sampler achieves quadratic convergence. The proposed method, incorporating posterior distributions and logit mixing, significantly outperforms existing approaches in empirical evaluations, suggesting new directions for algorithm design in language model alignment.

### 356. [Adversarial Suffixes May Be Features Too!](https://arxiv.org/pdf/2410.00451)
**Summary**: The paper investigates the hypothesis that adversarial suffixes in large language models (LLMs) are not just bugs but may represent features that can dominate the model's behavior, potentially compromising safety alignment. Through experiments, the authors demonstrate that benign features can be transformed into adversarial suffixes, and that these suffixes can introduce safety-compromising characteristics even when fine-tuning with benign datasets. The findings underscore the need for further research to enhance LLM safety alignment.

### 357. [Few-shot Prompting for Pairwise Ranking: An Effective Non-Parametric Retrieval Model](https://arxiv.org/pdf/2409.17745)
**Summary**: The paper introduces a pairwise few-shot ranker that enhances retrieval performance by leveraging a small number of training examples, showing improvements over zero-shot baselines in both in-domain and out-domain benchmarks. This method achieves results close to supervised models without the need for complex training pipelines.

### 358. [Representation Tuning](https://arxiv.org/pdf/2409.06927)
**Summary**: The paper introduces "representation tuning," a method for embedding behavioral vectors directly into large language models (LLMs) to control their output without the need for online adjustments. By fine-tuning LLMs like Llama-2-13b-chat using a dual loss function combining cosine similarity and token-based loss, the authors demonstrate enhanced control over honesty in model responses, outperforming both online steering and standard fine-tuning approaches. This method shows promise as a safety measure for LLMs.

### 359. [MaPPER: Multimodal Prior-guided Parameter Efficient Tuning for Referring Expression Comprehension](https://arxiv.org/pdf/2409.13609)
**Summary**: The paper introduces MaPPER, a novel framework for Referring Expression Comprehension (REC) that leverages Multimodal Prior-guided Parameter Efficient Tuning to enhance performance while significantly reducing computational costs. MaPPER uses Dynamic Prior Adapters and Local Convolution Adapters to improve local visual perception and cross-modal alignment, outperforming full fine-tuning and other Parameter-Efficient Transfer Learning methods with minimal parameter adjustments.

### 360. [Training Nonlinear Transformers for Chain-of-Thought Inference: A Theoretical Generalization Analysis](https://arxiv.org/pdf/2410.02167)
**Summary**: The paper presents a theoretical analysis of training nonlinear Transformers for Chain-of-Thought (CoT) inference, addressing the challenges of nonconvex optimization in nonlinear attention models. It quantifies the necessary training samples and iterations for achieving CoT generalization, proving its effectiveness on unseen tasks with distribution-shifted data, and characterizing conditions for accurate reasoning even with noisy examples. This contrasts with in-context learning, which may fail in similar scenarios.

### 361. [Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse Representation Adjustment in Large Language Models](https://arxiv.org/pdf/2410.02298)
**Summary**: The paper introduces Jailbreak Antidote, a method for dynamically adjusting the safety-utility balance in large language models (LLMs) by manipulating a sparse subset of the model's internal states during inference. This approach allows for real-time control over safety preferences without increasing computational overhead or inference latency, and it is shown to be effective across a range of LLMs and against various jailbreak attacks.

### 362. [Frame-Voyager: Learning to Query Frames for Video Large Language Models](https://arxiv.org/pdf/2410.03226)
**Summary**: The paper introduces Frame-Voyager, a method that learns to select informative frame combinations from videos for Video Large Language Models (Video-LLMs), addressing the limitation of input token length. By ranking frame combinations based on prediction losses from a pre-trained Video-LLM, Frame-Voyager is trained to query the most relevant frames, significantly improving performance in Video Question Answering benchmarks across different Video-LLMs.

### 363. [MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation](https://arxiv.org/pdf/2410.02458)
**Summary**: The paper introduces MedVisionLlama, a method that integrates pre-trained Large Language Model (LLM) transformer blocks into Vision Transformers (ViTs) to enhance medical image segmentation. By incorporating a frozen LLM transformer block in the encoder and using a Hybrid Attention Mechanism with Multi-Scale Fusion, the model achieves significant improvements in segmentation performance, with an average Dice score increase from 0.74 to 0.79 and enhancements in accuracy, precision, and the Jaccard Index.



---

*Last updated on 2024-10-09*